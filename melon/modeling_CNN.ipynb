{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os\n",
    "\n",
    "# 앨범 이미지를 불러와서 시대별로 나누기\n",
    "# 1990년대 : 0\n",
    "# 2000년대 : 1\n",
    "# 2010년대 : 2\n",
    "\n",
    "# 일부 이미지 3채널이 아니라 1채널(GrayScale)임\n",
    "# 우선\n",
    "filepath = './album/'\n",
    "imgList = glob.glob(filepath+\"*\")\n",
    "yearLabel = {'9': 0,\n",
    "             '0': 1,\n",
    "             '1': 2}\n",
    "\n",
    "imgdata = []\n",
    "eraLabel = []\n",
    "\n",
    "for path_ in imgList:\n",
    "    img = Image.open(path_)\n",
    "\n",
    "    # RGB 일 때\n",
    "    img = np.array(img).reshape(-1)\n",
    "    if len(img) == 6912:\n",
    "        imgdata.append(img)\n",
    "\n",
    "        era = path_.split('/')[-1].split('_')[1][2]\n",
    "        eraLabel.append(yearLabel[era])\n",
    "\n",
    "    # # gray scale 일 때 사용\n",
    "    # img = img.convert('L')\n",
    "    # img = np.array(img).reshape(-1)\n",
    "    # if len(img) == 2304:\n",
    "    #     imgdata.append(img)\n",
    "    #     era = path_.split('/')[-1].split('_')[1][2]\n",
    "    #     eraLabel.append(yearLabel[era])\n",
    "\n",
    "\n",
    "imgdata = np.array(imgdata).reshape((-1,48,48,3))\n",
    "eraLabel = np.array(eraLabel)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization,Activation,LeakyReLU,Conv2D,MaxPooling2D,AveragePooling2D,Flatten\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# 정규화 및 원핫인코딩\n",
    "imgdata = imgdata.astype('float32')/255.0\n",
    "eraLabel = to_categorical(eraLabel).astype(int)\n",
    "\n",
    "# 모델 생성\n",
    "folderpath = './models/'\n",
    "if not os.path.exists(folderpath):\n",
    "    os.makedirs(folderpath)\n",
    "    print('models 폴더 생성 완료')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cnn = Sequential(name='CNN_BatchNormalization_padding_same')\n",
    "\n",
    "cnn.add(Conv2D(64,kernel_size=3,input_shape=(48,48,3),activation='relu',kernel_initializer='he_normal',padding='same'))\n",
    "cnn.add(Conv2D(64,kernel_size=3,kernel_initializer='he_normal',padding='same'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(MaxPooling2D(padding='same'))\n",
    "\n",
    "# cnn.add(Conv2D(128,kernel_size=3,kernel_initializer='he_normal',padding='same'))\n",
    "# cnn.add(Conv2D(128,kernel_size=3,kernel_initializer='he_normal',padding='same'))\n",
    "# cnn.add(BatchNormalization())\n",
    "# cnn.add(Activation('relu'))\n",
    "\n",
    "cnn.add(Conv2D(128,kernel_size=3,kernel_initializer='he_normal',padding='same'))\n",
    "cnn.add(Conv2D(128,kernel_size=3,kernel_initializer='he_normal',padding='same'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(MaxPooling2D())\n",
    "cnn.add(Conv2D(256,kernel_size=3,kernel_initializer='he_normal',padding='same'))\n",
    "cnn.add(Conv2D(256,kernel_size=3,kernel_initializer='he_normal',padding='same'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(MaxPooling2D())\n",
    "\n",
    "cnn.add(Flatten())\n",
    "\n",
    "cnn.add(Dense(256,kernel_initializer='he_normal'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Activation('relu'))\n",
    "\n",
    "cnn.add(Dense(3,activation='softmax'))\n",
    "\n",
    "plot_model(cnn,to_file=f'{folderpath}{cnn.name}.png',show_layer_names=True,show_layer_activations=True,show_shapes=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "165/166 [============================>.] - ETA: 0s - loss: 1.0998 - accuracy: 0.3201\n",
      "Epoch 1: val_accuracy improved from -inf to 0.32993, saving model to ./models/CNN_BatchNormalization_padding_same.hdf5\n",
      "166/166 [==============================] - 31s 182ms/step - loss: 1.0998 - accuracy: 0.3197 - val_loss: 1.0987 - val_accuracy: 0.3299\n",
      "Epoch 2/50\n",
      "165/166 [============================>.] - ETA: 0s - loss: 1.0995 - accuracy: 0.3307\n",
      "Epoch 2: val_accuracy did not improve from 0.32993\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 1.0995 - accuracy: 0.3303 - val_loss: 1.0994 - val_accuracy: 0.3265\n",
      "Epoch 3/50\n",
      "165/166 [============================>.] - ETA: 0s - loss: 1.1001 - accuracy: 0.3333\n",
      "Epoch 3: val_accuracy did not improve from 0.32993\n",
      "166/166 [==============================] - 31s 188ms/step - loss: 1.1001 - accuracy: 0.3330 - val_loss: 1.0991 - val_accuracy: 0.3299\n",
      "Epoch 4/50\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.0995 - accuracy: 0.3288\n",
      "Epoch 4: val_accuracy did not improve from 0.32993\n",
      "166/166 [==============================] - 31s 189ms/step - loss: 1.0995 - accuracy: 0.3288 - val_loss: 1.0988 - val_accuracy: 0.3265\n",
      "Epoch 5/50\n",
      " 85/166 [==============>...............] - ETA: 15s - loss: 1.0997 - accuracy: 0.3044"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 14\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# def scheduler(epoch,lr=0.01):\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m#     if epoch % 10 == 0:\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m#         return lr * np.math.exp(-0.1)\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m#     else:\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m#         return lr\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# lrCB = LearningRateScheduler(schedule=scheduler)\u001B[39;00m\n\u001B[1;32m     11\u001B[0m cnn\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategorical_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m,optimizer\u001B[38;5;241m=\u001B[39mAdam(learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m),\n\u001B[1;32m     12\u001B[0m             metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m---> 14\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mcnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimgdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43meraLabel\u001B[49m\u001B[43m,\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43merCB\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmcCB\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \n\u001B[1;32m     18\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# 검증셋과 학습셋의 오차를 저장\u001B[39;00m\n\u001B[1;32m     23\u001B[0m y_vAcc \u001B[38;5;241m=\u001B[39m history\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_accuracy\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m~/anaconda3/envs/deep/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/anaconda3/envs/deep/lib/python3.9/site-packages/keras/engine/training.py:1685\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1677\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[1;32m   1678\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1679\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1682\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   1683\u001B[0m ):\n\u001B[1;32m   1684\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m-> 1685\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1686\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[1;32m   1687\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m~/anaconda3/envs/deep/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/anaconda3/envs/deep/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    891\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    893\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 894\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    896\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    897\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/anaconda3/envs/deep/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    923\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    924\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    925\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 926\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_no_variable_creation_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[1;32m    927\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    928\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    929\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[1;32m    930\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m~/anaconda3/envs/deep/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001B[0m, in \u001B[0;36mTracingCompiler.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    141\u001B[0m   (concrete_function,\n\u001B[1;32m    142\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[0;32m--> 143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconcrete_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    144\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconcrete_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/deep/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1753\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1754\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1755\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1756\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1757\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1758\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1759\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1760\u001B[0m     args,\n\u001B[1;32m   1761\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1762\u001B[0m     executing_eagerly)\n\u001B[1;32m   1763\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m~/anaconda3/envs/deep/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    379\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    380\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 381\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    382\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    383\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    384\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    385\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    387\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    388\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m    389\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[1;32m    390\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    393\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[1;32m    394\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[0;32m~/anaconda3/envs/deep/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 52\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     55\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam,nadam\n",
    "erCB = EarlyStopping(verbose=1,patience=20,monitor='val_accuracy')\n",
    "mcCB = ModelCheckpoint(folderpath+f\"{cnn.name}.hdf5\",save_best_only=True,monitor='val_accuracy',verbose=1)\n",
    "# def scheduler(epoch,lr=0.01):\n",
    "#     if epoch % 10 == 0:\n",
    "#         return lr * np.math.exp(-0.1)\n",
    "#     else:\n",
    "#         return lr\n",
    "# lrCB = LearningRateScheduler(schedule=scheduler)\n",
    "\n",
    "cnn.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=0.01),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "history = cnn.fit(imgdata,eraLabel,batch_size=16,epochs=50,\n",
    "        validation_split=0.1,\n",
    "            callbacks=[erCB,mcCB],\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "# 검증셋과 학습셋의 오차를 저장\n",
    "y_vAcc = history.history['val_accuracy']\n",
    "y_Acc = history.history['accuracy']\n",
    "\n",
    "# 그래프로 표현해 봅니다\n",
    "x_len = np.arange(len(y_Acc))\n",
    "plt.plot(x_len,y_vAcc,marker='.',c='red',label='Test_acc')\n",
    "plt.plot(x_len,y_Acc,marker='.',c='blue',label='Train_acc')\n",
    "\n",
    "# 그래프에 그리드를 주고 레이블 표시\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.savefig(f'././image/CNN_{cnn.name}_acc.jpg')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 검증셋과 학습셋의 오차를 저장\n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "# 그래프로 표현해 봅니다\n",
    "x_len = np.arange(len(y_Acc))\n",
    "plt.plot(x_len,y_vloss,marker='.',c='red',label='Test_loss')\n",
    "plt.plot(x_len,y_loss,marker='.',c='blue',label='Train_loss')\n",
    "\n",
    "# 그래프에 그리드를 주고 레이블 표시\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig(f'././image/CNN_{cnn.name}_loss.jpg')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def imgResize(filepath, size):\n",
    "    from PIL import Image\n",
    "\n",
    "    img = np.array(Image.open(filepath)).reshape(-1)\n",
    "    img = img.reshape(size)/255.0\n",
    "\n",
    "    return img.reshape((-1,48,48,3))\n",
    "\n",
    "\n",
    "from keras.applications import InceptionResNetV2\n",
    "InceptionResNetV2()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.4691 - accuracy: 0.3378\n",
      "Epoch 1: val_accuracy improved from -inf to 0.41327, saving model to ./models/ResNet50_Dense_add.hdf5\n",
      "147/147 [==============================] - 11s 66ms/step - loss: 1.4677 - accuracy: 0.3380 - val_loss: 1.1055 - val_accuracy: 0.4133 - lr: 9.0484e-04\n",
      "Epoch 2/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.2758 - accuracy: 0.3614\n",
      "Epoch 2: val_accuracy did not improve from 0.41327\n",
      "147/147 [==============================] - 10s 68ms/step - loss: 1.2758 - accuracy: 0.3614 - val_loss: 1.0861 - val_accuracy: 0.4031 - lr: 9.0484e-04\n",
      "Epoch 3/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.2296 - accuracy: 0.3542\n",
      "Epoch 3: val_accuracy did not improve from 0.41327\n",
      "147/147 [==============================] - 8s 58ms/step - loss: 1.2296 - accuracy: 0.3542 - val_loss: 1.0629 - val_accuracy: 0.4116 - lr: 9.0484e-04\n",
      "Epoch 4/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.1626 - accuracy: 0.3946\n",
      "Epoch 4: val_accuracy improved from 0.41327 to 0.44898, saving model to ./models/ResNet50_Dense_add.hdf5\n",
      "147/147 [==============================] - 8s 58ms/step - loss: 1.1626 - accuracy: 0.3946 - val_loss: 1.0600 - val_accuracy: 0.4490 - lr: 9.0484e-04\n",
      "Epoch 5/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.1390 - accuracy: 0.3780\n",
      "Epoch 5: val_accuracy improved from 0.44898 to 0.45068, saving model to ./models/ResNet50_Dense_add.hdf5\n",
      "147/147 [==============================] - 9s 59ms/step - loss: 1.1389 - accuracy: 0.3772 - val_loss: 1.0578 - val_accuracy: 0.4507 - lr: 9.0484e-04\n",
      "Epoch 6/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.1194 - accuracy: 0.3742\n",
      "Epoch 6: val_accuracy improved from 0.45068 to 0.46429, saving model to ./models/ResNet50_Dense_add.hdf5\n",
      "147/147 [==============================] - 9s 60ms/step - loss: 1.1194 - accuracy: 0.3742 - val_loss: 1.0654 - val_accuracy: 0.4643 - lr: 9.0484e-04\n",
      "Epoch 7/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.1179 - accuracy: 0.3669\n",
      "Epoch 7: val_accuracy did not improve from 0.46429\n",
      "147/147 [==============================] - 9s 60ms/step - loss: 1.1179 - accuracy: 0.3665 - val_loss: 1.0589 - val_accuracy: 0.4524 - lr: 9.0484e-04\n",
      "Epoch 8/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.0993 - accuracy: 0.3819\n",
      "Epoch 8: val_accuracy did not improve from 0.46429\n",
      "147/147 [==============================] - 9s 61ms/step - loss: 1.0993 - accuracy: 0.3819 - val_loss: 1.0500 - val_accuracy: 0.4592 - lr: 9.0484e-04\n",
      "Epoch 9/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0960 - accuracy: 0.3750\n",
      "Epoch 9: val_accuracy improved from 0.46429 to 0.47279, saving model to ./models/ResNet50_Dense_add.hdf5\n",
      "147/147 [==============================] - 9s 63ms/step - loss: 1.0959 - accuracy: 0.3746 - val_loss: 1.0461 - val_accuracy: 0.4728 - lr: 9.0484e-04\n",
      "Epoch 10/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0790 - accuracy: 0.4033\n",
      "Epoch 10: val_accuracy improved from 0.47279 to 0.47789, saving model to ./models/ResNet50_Dense_add.hdf5\n",
      "147/147 [==============================] - 9s 64ms/step - loss: 1.0797 - accuracy: 0.4019 - val_loss: 1.0540 - val_accuracy: 0.4779 - lr: 9.0484e-04\n",
      "Epoch 11/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0657 - accuracy: 0.4187\n",
      "Epoch 11: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 9s 63ms/step - loss: 1.0672 - accuracy: 0.4176 - val_loss: 1.0462 - val_accuracy: 0.4558 - lr: 9.0484e-04\n",
      "Epoch 12/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0693 - accuracy: 0.4255\n",
      "Epoch 12: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 9s 64ms/step - loss: 1.0695 - accuracy: 0.4253 - val_loss: 1.0447 - val_accuracy: 0.4694 - lr: 9.0484e-04\n",
      "Epoch 13/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0622 - accuracy: 0.4195\n",
      "Epoch 13: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 9s 64ms/step - loss: 1.0630 - accuracy: 0.4185 - val_loss: 1.0440 - val_accuracy: 0.4626 - lr: 9.0484e-04\n",
      "Epoch 14/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0685 - accuracy: 0.4375\n",
      "Epoch 14: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 10s 67ms/step - loss: 1.0689 - accuracy: 0.4376 - val_loss: 1.0457 - val_accuracy: 0.4524 - lr: 9.0484e-04\n",
      "Epoch 15/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0589 - accuracy: 0.4443\n",
      "Epoch 15: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 10s 65ms/step - loss: 1.0590 - accuracy: 0.4436 - val_loss: 1.0341 - val_accuracy: 0.4592 - lr: 9.0484e-04\n",
      "Epoch 16/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0492 - accuracy: 0.4443\n",
      "Epoch 16: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 10s 65ms/step - loss: 1.0495 - accuracy: 0.4440 - val_loss: 1.0427 - val_accuracy: 0.4422 - lr: 9.0484e-04\n",
      "Epoch 17/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0476 - accuracy: 0.4555\n",
      "Epoch 17: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 10s 66ms/step - loss: 1.0470 - accuracy: 0.4559 - val_loss: 1.0589 - val_accuracy: 0.4371 - lr: 9.0484e-04\n",
      "Epoch 18/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0412 - accuracy: 0.4538\n",
      "Epoch 18: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 10s 65ms/step - loss: 1.0414 - accuracy: 0.4534 - val_loss: 1.0539 - val_accuracy: 0.4405 - lr: 9.0484e-04\n",
      "Epoch 19/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0447 - accuracy: 0.4632\n",
      "Epoch 19: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 9s 65ms/step - loss: 1.0442 - accuracy: 0.4640 - val_loss: 1.0517 - val_accuracy: 0.4337 - lr: 9.0484e-04\n",
      "Epoch 20/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0443 - accuracy: 0.4443\n",
      "Epoch 20: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 9s 64ms/step - loss: 1.0445 - accuracy: 0.4440 - val_loss: 1.0324 - val_accuracy: 0.4609 - lr: 9.0484e-04\n",
      "Epoch 21/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0357 - accuracy: 0.4688\n",
      "Epoch 21: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 10s 65ms/step - loss: 1.0351 - accuracy: 0.4696 - val_loss: 1.0383 - val_accuracy: 0.4524 - lr: 9.0484e-04\n",
      "Epoch 22/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0446 - accuracy: 0.4555\n",
      "Epoch 22: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 10s 66ms/step - loss: 1.0454 - accuracy: 0.4547 - val_loss: 1.0346 - val_accuracy: 0.4575 - lr: 9.0484e-04\n",
      "Epoch 23/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0298 - accuracy: 0.4722\n",
      "Epoch 23: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 10s 66ms/step - loss: 1.0304 - accuracy: 0.4713 - val_loss: 1.0536 - val_accuracy: 0.4201 - lr: 9.0484e-04\n",
      "Epoch 24/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0309 - accuracy: 0.4705\n",
      "Epoch 24: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 10s 67ms/step - loss: 1.0305 - accuracy: 0.4704 - val_loss: 1.0274 - val_accuracy: 0.4779 - lr: 9.0484e-04\n",
      "Epoch 25/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0312 - accuracy: 0.4747\n",
      "Epoch 25: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 10s 67ms/step - loss: 1.0322 - accuracy: 0.4730 - val_loss: 1.0377 - val_accuracy: 0.4677 - lr: 9.0484e-04\n",
      "Epoch 26/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4705\n",
      "Epoch 26: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 10s 66ms/step - loss: 1.0347 - accuracy: 0.4704 - val_loss: 1.0278 - val_accuracy: 0.4643 - lr: 9.0484e-04\n",
      "Epoch 27/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0151 - accuracy: 0.4816\n",
      "Epoch 27: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 10s 66ms/step - loss: 1.0152 - accuracy: 0.4815 - val_loss: 1.0305 - val_accuracy: 0.4490 - lr: 9.0484e-04\n",
      "Epoch 28/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0141 - accuracy: 0.4807\n",
      "Epoch 28: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 10s 66ms/step - loss: 1.0131 - accuracy: 0.4815 - val_loss: 1.0317 - val_accuracy: 0.4694 - lr: 9.0484e-04\n",
      "Epoch 29/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0222 - accuracy: 0.4662\n",
      "Epoch 29: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 10s 66ms/step - loss: 1.0214 - accuracy: 0.4674 - val_loss: 1.0271 - val_accuracy: 0.4609 - lr: 9.0484e-04\n",
      "Epoch 30/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0112 - accuracy: 0.5077\n",
      "Epoch 30: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 10s 66ms/step - loss: 1.0103 - accuracy: 0.5087 - val_loss: 1.0533 - val_accuracy: 0.4286 - lr: 9.0484e-04\n",
      "Epoch 31/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0092 - accuracy: 0.5017\n",
      "Epoch 31: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 10s 66ms/step - loss: 1.0093 - accuracy: 0.5011 - val_loss: 1.0305 - val_accuracy: 0.4660 - lr: 9.0484e-04\n",
      "Epoch 32/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0091 - accuracy: 0.4932\n",
      "Epoch 32: val_accuracy did not improve from 0.47789\n",
      "147/147 [==============================] - 10s 67ms/step - loss: 1.0094 - accuracy: 0.4930 - val_loss: 1.0280 - val_accuracy: 0.4728 - lr: 9.0484e-04\n",
      "Epoch 33/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0058 - accuracy: 0.5009\n",
      "Epoch 33: val_accuracy improved from 0.47789 to 0.48299, saving model to ./models/ResNet50_Dense_add.hdf5\n",
      "147/147 [==============================] - 10s 68ms/step - loss: 1.0053 - accuracy: 0.5011 - val_loss: 1.0208 - val_accuracy: 0.4830 - lr: 9.0484e-04\n",
      "Epoch 34/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0044 - accuracy: 0.5034\n",
      "Epoch 34: val_accuracy did not improve from 0.48299\n",
      "147/147 [==============================] - 10s 67ms/step - loss: 1.0047 - accuracy: 0.5036 - val_loss: 1.0334 - val_accuracy: 0.4575 - lr: 9.0484e-04\n",
      "Epoch 35/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0074 - accuracy: 0.5034\n",
      "Epoch 35: val_accuracy did not improve from 0.48299\n",
      "147/147 [==============================] - 10s 69ms/step - loss: 1.0067 - accuracy: 0.5040 - val_loss: 1.0225 - val_accuracy: 0.4575 - lr: 9.0484e-04\n",
      "Epoch 36/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9922 - accuracy: 0.5133\n",
      "Epoch 36: val_accuracy did not improve from 0.48299\n",
      "147/147 [==============================] - 10s 71ms/step - loss: 0.9927 - accuracy: 0.5130 - val_loss: 1.0752 - val_accuracy: 0.4218 - lr: 9.0484e-04\n",
      "Epoch 37/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 1.0052 - accuracy: 0.5034\n",
      "Epoch 37: val_accuracy improved from 0.48299 to 0.49490, saving model to ./models/ResNet50_Dense_add.hdf5\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 1.0051 - accuracy: 0.5036 - val_loss: 1.0174 - val_accuracy: 0.4949 - lr: 9.0484e-04\n",
      "Epoch 38/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9762 - accuracy: 0.5274\n",
      "Epoch 38: val_accuracy did not improve from 0.49490\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.9758 - accuracy: 0.5279 - val_loss: 1.0294 - val_accuracy: 0.4694 - lr: 9.0484e-04\n",
      "Epoch 39/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9717 - accuracy: 0.5317\n",
      "Epoch 39: val_accuracy did not improve from 0.49490\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.9720 - accuracy: 0.5317 - val_loss: 1.0327 - val_accuracy: 0.4677 - lr: 9.0484e-04\n",
      "Epoch 40/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9848 - accuracy: 0.5201\n",
      "Epoch 40: val_accuracy did not improve from 0.49490\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.9842 - accuracy: 0.5198 - val_loss: 1.0800 - val_accuracy: 0.4286 - lr: 9.0484e-04\n",
      "Epoch 41/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9721 - accuracy: 0.5437\n",
      "Epoch 41: val_accuracy did not improve from 0.49490\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.9724 - accuracy: 0.5428 - val_loss: 1.1144 - val_accuracy: 0.4592 - lr: 9.0484e-04\n",
      "Epoch 42/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9840 - accuracy: 0.5193\n",
      "Epoch 42: val_accuracy did not improve from 0.49490\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.9841 - accuracy: 0.5194 - val_loss: 1.0132 - val_accuracy: 0.4864 - lr: 9.0484e-04\n",
      "Epoch 43/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9714 - accuracy: 0.5295\n",
      "Epoch 43: val_accuracy did not improve from 0.49490\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.9707 - accuracy: 0.5300 - val_loss: 1.0506 - val_accuracy: 0.4439 - lr: 9.0484e-04\n",
      "Epoch 44/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9791 - accuracy: 0.5304\n",
      "Epoch 44: val_accuracy did not improve from 0.49490\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.9798 - accuracy: 0.5304 - val_loss: 1.1701 - val_accuracy: 0.4694 - lr: 9.0484e-04\n",
      "Epoch 45/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9524 - accuracy: 0.5385\n",
      "Epoch 45: val_accuracy improved from 0.49490 to 0.52041, saving model to ./models/ResNet50_Dense_add.hdf5\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.9533 - accuracy: 0.5385 - val_loss: 0.9836 - val_accuracy: 0.5204 - lr: 9.0484e-04\n",
      "Epoch 46/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9665 - accuracy: 0.5432\n",
      "Epoch 46: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 10s 71ms/step - loss: 0.9659 - accuracy: 0.5432 - val_loss: 1.0067 - val_accuracy: 0.4915 - lr: 9.0484e-04\n",
      "Epoch 47/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9407 - accuracy: 0.5497\n",
      "Epoch 47: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.9402 - accuracy: 0.5500 - val_loss: 1.0579 - val_accuracy: 0.4762 - lr: 9.0484e-04\n",
      "Epoch 48/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9682 - accuracy: 0.5368\n",
      "Epoch 48: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 10s 71ms/step - loss: 0.9688 - accuracy: 0.5355 - val_loss: 1.0094 - val_accuracy: 0.4830 - lr: 9.0484e-04\n",
      "Epoch 49/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9525 - accuracy: 0.5484\n",
      "Epoch 49: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 10s 71ms/step - loss: 0.9521 - accuracy: 0.5475 - val_loss: 1.0311 - val_accuracy: 0.4592 - lr: 9.0484e-04\n",
      "Epoch 50/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9570 - accuracy: 0.5394\n",
      "Epoch 50: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 10s 71ms/step - loss: 0.9560 - accuracy: 0.5411 - val_loss: 1.0146 - val_accuracy: 0.4813 - lr: 9.0484e-04\n",
      "Epoch 51/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9418 - accuracy: 0.5552\n",
      "Epoch 51: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.9425 - accuracy: 0.5547 - val_loss: 1.0659 - val_accuracy: 0.4694 - lr: 9.0484e-04\n",
      "Epoch 52/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9198 - accuracy: 0.5634\n",
      "Epoch 52: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.9200 - accuracy: 0.5641 - val_loss: 1.0167 - val_accuracy: 0.4813 - lr: 9.0484e-04\n",
      "Epoch 53/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9130 - accuracy: 0.5792\n",
      "Epoch 53: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 10s 71ms/step - loss: 0.9132 - accuracy: 0.5790 - val_loss: 1.0390 - val_accuracy: 0.4575 - lr: 9.0484e-04\n",
      "Epoch 54/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9236 - accuracy: 0.5646\n",
      "Epoch 54: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.9246 - accuracy: 0.5632 - val_loss: 1.0637 - val_accuracy: 0.4439 - lr: 9.0484e-04\n",
      "Epoch 55/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9105 - accuracy: 0.5758\n",
      "Epoch 55: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.9097 - accuracy: 0.5764 - val_loss: 1.1514 - val_accuracy: 0.4422 - lr: 9.0484e-04\n",
      "Epoch 56/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9210 - accuracy: 0.5741\n",
      "Epoch 56: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 10s 71ms/step - loss: 0.9208 - accuracy: 0.5751 - val_loss: 1.0389 - val_accuracy: 0.4762 - lr: 9.0484e-04\n",
      "Epoch 57/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9086 - accuracy: 0.5967\n",
      "Epoch 57: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.9088 - accuracy: 0.5977 - val_loss: 1.0805 - val_accuracy: 0.4524 - lr: 9.0484e-04\n",
      "Epoch 58/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.9076 - accuracy: 0.5822\n",
      "Epoch 58: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.9062 - accuracy: 0.5828 - val_loss: 1.0138 - val_accuracy: 0.4728 - lr: 9.0484e-04\n",
      "Epoch 59/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.8839 - accuracy: 0.5920\n",
      "Epoch 59: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.8862 - accuracy: 0.5905 - val_loss: 1.2792 - val_accuracy: 0.3912 - lr: 9.0484e-04\n",
      "Epoch 60/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.8934 - accuracy: 0.5882\n",
      "Epoch 60: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.8927 - accuracy: 0.5888 - val_loss: 1.0624 - val_accuracy: 0.4626 - lr: 9.0484e-04\n",
      "Epoch 61/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.8922 - accuracy: 0.5938\n",
      "Epoch 61: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 10s 71ms/step - loss: 0.8930 - accuracy: 0.5934 - val_loss: 1.0381 - val_accuracy: 0.4609 - lr: 9.0484e-04\n",
      "Epoch 62/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.8786 - accuracy: 0.6113\n",
      "Epoch 62: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.8789 - accuracy: 0.6109 - val_loss: 0.9958 - val_accuracy: 0.5119 - lr: 9.0484e-04\n",
      "Epoch 63/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.8809 - accuracy: 0.6019\n",
      "Epoch 63: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.8806 - accuracy: 0.6024 - val_loss: 1.0614 - val_accuracy: 0.4745 - lr: 9.0484e-04\n",
      "Epoch 64/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.8718 - accuracy: 0.6126\n",
      "Epoch 64: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.8718 - accuracy: 0.6122 - val_loss: 1.0971 - val_accuracy: 0.4796 - lr: 9.0484e-04\n",
      "Epoch 65/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.8603 - accuracy: 0.6036\n",
      "Epoch 65: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.8597 - accuracy: 0.6041 - val_loss: 1.0535 - val_accuracy: 0.5068 - lr: 9.0484e-04\n",
      "Epoch 66/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.8520 - accuracy: 0.6100\n",
      "Epoch 66: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.8502 - accuracy: 0.6109 - val_loss: 1.0355 - val_accuracy: 0.5136 - lr: 9.0484e-04\n",
      "Epoch 67/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.8507 - accuracy: 0.6241\n",
      "Epoch 67: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.8511 - accuracy: 0.6237 - val_loss: 1.1147 - val_accuracy: 0.4609 - lr: 9.0484e-04\n",
      "Epoch 68/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.8458 - accuracy: 0.6164\n",
      "Epoch 68: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.8441 - accuracy: 0.6177 - val_loss: 1.0576 - val_accuracy: 0.4575 - lr: 9.0484e-04\n",
      "Epoch 69/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.8457 - accuracy: 0.6220\n",
      "Epoch 69: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.8457 - accuracy: 0.6220 - val_loss: 1.0111 - val_accuracy: 0.4898 - lr: 9.0484e-04\n",
      "Epoch 70/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.8539 - accuracy: 0.6203\n",
      "Epoch 70: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.8545 - accuracy: 0.6198 - val_loss: 1.0873 - val_accuracy: 0.5051 - lr: 9.0484e-04\n",
      "Epoch 71/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.8447 - accuracy: 0.6177\n",
      "Epoch 71: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.8463 - accuracy: 0.6173 - val_loss: 1.0868 - val_accuracy: 0.4966 - lr: 9.0484e-04\n",
      "Epoch 72/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.8531 - accuracy: 0.6220\n",
      "Epoch 72: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.8543 - accuracy: 0.6220 - val_loss: 0.9840 - val_accuracy: 0.5204 - lr: 9.0484e-04\n",
      "Epoch 73/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.8302 - accuracy: 0.6318\n",
      "Epoch 73: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.8306 - accuracy: 0.6318 - val_loss: 1.1271 - val_accuracy: 0.4813 - lr: 9.0484e-04\n",
      "Epoch 74/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.8105 - accuracy: 0.6336\n",
      "Epoch 74: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.8094 - accuracy: 0.6339 - val_loss: 1.0276 - val_accuracy: 0.5051 - lr: 9.0484e-04\n",
      "Epoch 75/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.8337 - accuracy: 0.6293\n",
      "Epoch 75: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.8328 - accuracy: 0.6301 - val_loss: 1.0342 - val_accuracy: 0.4762 - lr: 9.0484e-04\n",
      "Epoch 76/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.8161 - accuracy: 0.6344\n",
      "Epoch 76: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.8166 - accuracy: 0.6339 - val_loss: 1.0600 - val_accuracy: 0.5000 - lr: 9.0484e-04\n",
      "Epoch 77/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.8129 - accuracy: 0.6494\n",
      "Epoch 77: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.8143 - accuracy: 0.6488 - val_loss: 1.1038 - val_accuracy: 0.4966 - lr: 9.0484e-04\n",
      "Epoch 78/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.8141 - accuracy: 0.6310\n",
      "Epoch 78: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.8152 - accuracy: 0.6296 - val_loss: 1.0576 - val_accuracy: 0.5170 - lr: 9.0484e-04\n",
      "Epoch 79/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7959 - accuracy: 0.6468\n",
      "Epoch 79: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.7946 - accuracy: 0.6475 - val_loss: 1.0533 - val_accuracy: 0.5034 - lr: 9.0484e-04\n",
      "Epoch 80/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.8066 - accuracy: 0.6396\n",
      "Epoch 80: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.8068 - accuracy: 0.6394 - val_loss: 1.1332 - val_accuracy: 0.4320 - lr: 9.0484e-04\n",
      "Epoch 81/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7885 - accuracy: 0.6473\n",
      "Epoch 81: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.7887 - accuracy: 0.6471 - val_loss: 1.0221 - val_accuracy: 0.5000 - lr: 9.0484e-04\n",
      "Epoch 82/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7997 - accuracy: 0.6434\n",
      "Epoch 82: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.7998 - accuracy: 0.6424 - val_loss: 1.1627 - val_accuracy: 0.5017 - lr: 9.0484e-04\n",
      "Epoch 83/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.8007 - accuracy: 0.6408\n",
      "Epoch 83: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.8026 - accuracy: 0.6407 - val_loss: 1.0952 - val_accuracy: 0.4558 - lr: 9.0484e-04\n",
      "Epoch 84/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7948 - accuracy: 0.6622\n",
      "Epoch 84: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.7941 - accuracy: 0.6628 - val_loss: 1.1095 - val_accuracy: 0.4490 - lr: 9.0484e-04\n",
      "Epoch 85/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7794 - accuracy: 0.6571\n",
      "Epoch 85: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.7804 - accuracy: 0.6564 - val_loss: 1.1571 - val_accuracy: 0.4830 - lr: 9.0484e-04\n",
      "Epoch 86/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7859 - accuracy: 0.6622\n",
      "Epoch 86: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.7852 - accuracy: 0.6624 - val_loss: 1.0793 - val_accuracy: 0.5034 - lr: 9.0484e-04\n",
      "Epoch 87/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7956 - accuracy: 0.6528\n",
      "Epoch 87: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.7948 - accuracy: 0.6526 - val_loss: 1.0228 - val_accuracy: 0.5187 - lr: 9.0484e-04\n",
      "Epoch 88/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7728 - accuracy: 0.6627\n",
      "Epoch 88: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.7742 - accuracy: 0.6624 - val_loss: 1.2610 - val_accuracy: 0.5034 - lr: 9.0484e-04\n",
      "Epoch 89/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7637 - accuracy: 0.6699\n",
      "Epoch 89: val_accuracy did not improve from 0.52041\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.7628 - accuracy: 0.6713 - val_loss: 1.1099 - val_accuracy: 0.4932 - lr: 9.0484e-04\n",
      "Epoch 90/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7788 - accuracy: 0.6588\n",
      "Epoch 90: val_accuracy improved from 0.52041 to 0.53401, saving model to ./models/ResNet50_Dense_add.hdf5\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.7801 - accuracy: 0.6582 - val_loss: 1.0158 - val_accuracy: 0.5340 - lr: 9.0484e-04\n",
      "Epoch 91/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7852 - accuracy: 0.6498\n",
      "Epoch 91: val_accuracy did not improve from 0.53401\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.7851 - accuracy: 0.6492 - val_loss: 1.1194 - val_accuracy: 0.5000 - lr: 9.0484e-04\n",
      "Epoch 92/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7664 - accuracy: 0.6592\n",
      "Epoch 92: val_accuracy did not improve from 0.53401\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.7655 - accuracy: 0.6594 - val_loss: 1.2225 - val_accuracy: 0.4439 - lr: 9.0484e-04\n",
      "Epoch 93/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7774 - accuracy: 0.6635\n",
      "Epoch 93: val_accuracy did not improve from 0.53401\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.7778 - accuracy: 0.6633 - val_loss: 1.1116 - val_accuracy: 0.5238 - lr: 9.0484e-04\n",
      "Epoch 94/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7596 - accuracy: 0.6755\n",
      "Epoch 94: val_accuracy did not improve from 0.53401\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.7591 - accuracy: 0.6752 - val_loss: 1.1443 - val_accuracy: 0.4949 - lr: 9.0484e-04\n",
      "Epoch 95/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7445 - accuracy: 0.6811\n",
      "Epoch 95: val_accuracy did not improve from 0.53401\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.7468 - accuracy: 0.6799 - val_loss: 1.1533 - val_accuracy: 0.5136 - lr: 9.0484e-04\n",
      "Epoch 96/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7552 - accuracy: 0.6640\n",
      "Epoch 96: val_accuracy did not improve from 0.53401\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.7548 - accuracy: 0.6641 - val_loss: 1.1625 - val_accuracy: 0.4881 - lr: 9.0484e-04\n",
      "Epoch 97/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7398 - accuracy: 0.6849\n",
      "Epoch 97: val_accuracy improved from 0.53401 to 0.54252, saving model to ./models/ResNet50_Dense_add.hdf5\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.7410 - accuracy: 0.6845 - val_loss: 1.0430 - val_accuracy: 0.5425 - lr: 9.0484e-04\n",
      "Epoch 98/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.7455 - accuracy: 0.6748\n",
      "Epoch 98: val_accuracy did not improve from 0.54252\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.7455 - accuracy: 0.6748 - val_loss: 1.0487 - val_accuracy: 0.5170 - lr: 9.0484e-04\n",
      "Epoch 99/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7207 - accuracy: 0.6871\n",
      "Epoch 99: val_accuracy improved from 0.54252 to 0.54422, saving model to ./models/ResNet50_Dense_add.hdf5\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.7195 - accuracy: 0.6880 - val_loss: 1.0123 - val_accuracy: 0.5442 - lr: 9.0484e-04\n",
      "Epoch 100/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7148 - accuracy: 0.6943\n",
      "Epoch 100: val_accuracy did not improve from 0.54422\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.7138 - accuracy: 0.6948 - val_loss: 1.1921 - val_accuracy: 0.4711 - lr: 9.0484e-04\n",
      "Epoch 101/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7332 - accuracy: 0.6858\n",
      "Epoch 101: val_accuracy did not improve from 0.54422\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.7349 - accuracy: 0.6854 - val_loss: 1.0911 - val_accuracy: 0.4966 - lr: 8.1873e-04\n",
      "Epoch 102/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7094 - accuracy: 0.7012\n",
      "Epoch 102: val_accuracy did not improve from 0.54422\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.7084 - accuracy: 0.7020 - val_loss: 1.2539 - val_accuracy: 0.5051 - lr: 8.1873e-04\n",
      "Epoch 103/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.6985 - accuracy: 0.7008\n",
      "Epoch 103: val_accuracy did not improve from 0.54422\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.6987 - accuracy: 0.7007 - val_loss: 1.0372 - val_accuracy: 0.5323 - lr: 8.1873e-04\n",
      "Epoch 104/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.6986 - accuracy: 0.6939\n",
      "Epoch 104: val_accuracy did not improve from 0.54422\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.6978 - accuracy: 0.6948 - val_loss: 1.3046 - val_accuracy: 0.4847 - lr: 8.1873e-04\n",
      "Epoch 105/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7177 - accuracy: 0.6909\n",
      "Epoch 105: val_accuracy did not improve from 0.54422\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.7169 - accuracy: 0.6918 - val_loss: 1.0621 - val_accuracy: 0.5323 - lr: 8.1873e-04\n",
      "Epoch 106/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6997 - accuracy: 0.7011\n",
      "Epoch 106: val_accuracy did not improve from 0.54422\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.6997 - accuracy: 0.7011 - val_loss: 1.1649 - val_accuracy: 0.5170 - lr: 8.1873e-04\n",
      "Epoch 107/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7058 - accuracy: 0.6982\n",
      "Epoch 107: val_accuracy improved from 0.54422 to 0.55442, saving model to ./models/ResNet50_Dense_add.hdf5\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.7071 - accuracy: 0.6973 - val_loss: 1.1149 - val_accuracy: 0.5544 - lr: 8.1873e-04\n",
      "Epoch 108/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.7053 - accuracy: 0.6875\n",
      "Epoch 108: val_accuracy did not improve from 0.55442\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.7053 - accuracy: 0.6875 - val_loss: 1.0963 - val_accuracy: 0.5442 - lr: 8.1873e-04\n",
      "Epoch 109/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.7147 - accuracy: 0.6871\n",
      "Epoch 109: val_accuracy did not improve from 0.55442\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.7147 - accuracy: 0.6871 - val_loss: 1.3246 - val_accuracy: 0.4541 - lr: 8.1873e-04\n",
      "Epoch 110/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.7040 - accuracy: 0.7051\n",
      "Epoch 110: val_accuracy did not improve from 0.55442\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.7017 - accuracy: 0.7067 - val_loss: 1.2915 - val_accuracy: 0.4949 - lr: 8.1873e-04\n",
      "Epoch 111/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.6986 - accuracy: 0.6961\n",
      "Epoch 111: val_accuracy did not improve from 0.55442\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.6969 - accuracy: 0.6969 - val_loss: 1.2520 - val_accuracy: 0.4966 - lr: 8.1873e-04\n",
      "Epoch 112/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.6752 - accuracy: 0.7140\n",
      "Epoch 112: val_accuracy improved from 0.55442 to 0.56633, saving model to ./models/ResNet50_Dense_add.hdf5\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.6740 - accuracy: 0.7143 - val_loss: 1.0671 - val_accuracy: 0.5663 - lr: 8.1873e-04\n",
      "Epoch 113/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6755 - accuracy: 0.7199\n",
      "Epoch 113: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.6755 - accuracy: 0.7199 - val_loss: 1.1576 - val_accuracy: 0.4779 - lr: 8.1873e-04\n",
      "Epoch 114/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6739 - accuracy: 0.7195\n",
      "Epoch 114: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.6739 - accuracy: 0.7195 - val_loss: 1.0574 - val_accuracy: 0.5425 - lr: 8.1873e-04\n",
      "Epoch 115/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6843 - accuracy: 0.7105\n",
      "Epoch 115: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.6843 - accuracy: 0.7105 - val_loss: 1.0962 - val_accuracy: 0.5289 - lr: 8.1873e-04\n",
      "Epoch 116/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.6788 - accuracy: 0.7192\n",
      "Epoch 116: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.6807 - accuracy: 0.7182 - val_loss: 1.0812 - val_accuracy: 0.5578 - lr: 8.1873e-04\n",
      "Epoch 117/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6844 - accuracy: 0.7118\n",
      "Epoch 117: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.6844 - accuracy: 0.7118 - val_loss: 1.0388 - val_accuracy: 0.5425 - lr: 8.1873e-04\n",
      "Epoch 118/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6481 - accuracy: 0.7241\n",
      "Epoch 118: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.6481 - accuracy: 0.7241 - val_loss: 1.4881 - val_accuracy: 0.4490 - lr: 8.1873e-04\n",
      "Epoch 119/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6742 - accuracy: 0.7092\n",
      "Epoch 119: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.6742 - accuracy: 0.7092 - val_loss: 1.5137 - val_accuracy: 0.4150 - lr: 8.1873e-04\n",
      "Epoch 120/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6757 - accuracy: 0.7135\n",
      "Epoch 120: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.6757 - accuracy: 0.7135 - val_loss: 1.1872 - val_accuracy: 0.5493 - lr: 8.1873e-04\n",
      "Epoch 121/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6477 - accuracy: 0.7284\n",
      "Epoch 121: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.6477 - accuracy: 0.7284 - val_loss: 1.5346 - val_accuracy: 0.4660 - lr: 8.1873e-04\n",
      "Epoch 122/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6685 - accuracy: 0.7084\n",
      "Epoch 122: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.6685 - accuracy: 0.7084 - val_loss: 1.0938 - val_accuracy: 0.5374 - lr: 8.1873e-04\n",
      "Epoch 123/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6563 - accuracy: 0.7190\n",
      "Epoch 123: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.6563 - accuracy: 0.7190 - val_loss: 1.2602 - val_accuracy: 0.5289 - lr: 8.1873e-04\n",
      "Epoch 124/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6891 - accuracy: 0.7156\n",
      "Epoch 124: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.6891 - accuracy: 0.7156 - val_loss: 1.3797 - val_accuracy: 0.5170 - lr: 8.1873e-04\n",
      "Epoch 125/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6613 - accuracy: 0.7182\n",
      "Epoch 125: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.6613 - accuracy: 0.7182 - val_loss: 1.3548 - val_accuracy: 0.4983 - lr: 8.1873e-04\n",
      "Epoch 126/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6563 - accuracy: 0.7258\n",
      "Epoch 126: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.6563 - accuracy: 0.7258 - val_loss: 1.1817 - val_accuracy: 0.5374 - lr: 8.1873e-04\n",
      "Epoch 127/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6361 - accuracy: 0.7318\n",
      "Epoch 127: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.6361 - accuracy: 0.7318 - val_loss: 1.0448 - val_accuracy: 0.5476 - lr: 8.1873e-04\n",
      "Epoch 128/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6390 - accuracy: 0.7305\n",
      "Epoch 128: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.6390 - accuracy: 0.7305 - val_loss: 1.1949 - val_accuracy: 0.5646 - lr: 8.1873e-04\n",
      "Epoch 129/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6235 - accuracy: 0.7378\n",
      "Epoch 129: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.6235 - accuracy: 0.7378 - val_loss: 1.3504 - val_accuracy: 0.5000 - lr: 8.1873e-04\n",
      "Epoch 130/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6344 - accuracy: 0.7322\n",
      "Epoch 130: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.6344 - accuracy: 0.7322 - val_loss: 1.3384 - val_accuracy: 0.4881 - lr: 8.1873e-04\n",
      "Epoch 131/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6033 - accuracy: 0.7463\n",
      "Epoch 131: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.6033 - accuracy: 0.7463 - val_loss: 1.2786 - val_accuracy: 0.5204 - lr: 8.1873e-04\n",
      "Epoch 132/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6055 - accuracy: 0.7454\n",
      "Epoch 132: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.6055 - accuracy: 0.7454 - val_loss: 1.6239 - val_accuracy: 0.5068 - lr: 8.1873e-04\n",
      "Epoch 133/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6530 - accuracy: 0.7314\n",
      "Epoch 133: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.6530 - accuracy: 0.7314 - val_loss: 1.4543 - val_accuracy: 0.4354 - lr: 8.1873e-04\n",
      "Epoch 134/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6246 - accuracy: 0.7395\n",
      "Epoch 134: val_accuracy did not improve from 0.56633\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.6246 - accuracy: 0.7395 - val_loss: 1.4245 - val_accuracy: 0.5136 - lr: 8.1873e-04\n",
      "Epoch 135/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6208 - accuracy: 0.7433\n",
      "Epoch 135: val_accuracy improved from 0.56633 to 0.58333, saving model to ./models/ResNet50_Dense_add.hdf5\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.6208 - accuracy: 0.7433 - val_loss: 1.0634 - val_accuracy: 0.5833 - lr: 8.1873e-04\n",
      "Epoch 136/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6182 - accuracy: 0.7420\n",
      "Epoch 136: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.6182 - accuracy: 0.7420 - val_loss: 1.5152 - val_accuracy: 0.4439 - lr: 8.1873e-04\n",
      "Epoch 137/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6213 - accuracy: 0.7390\n",
      "Epoch 137: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.6213 - accuracy: 0.7390 - val_loss: 1.3905 - val_accuracy: 0.5000 - lr: 8.1873e-04\n",
      "Epoch 138/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6218 - accuracy: 0.7416\n",
      "Epoch 138: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.6218 - accuracy: 0.7416 - val_loss: 1.2760 - val_accuracy: 0.5510 - lr: 8.1873e-04\n",
      "Epoch 139/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6175 - accuracy: 0.7399\n",
      "Epoch 139: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.6175 - accuracy: 0.7399 - val_loss: 1.4754 - val_accuracy: 0.4609 - lr: 8.1873e-04\n",
      "Epoch 140/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6226 - accuracy: 0.7467\n",
      "Epoch 140: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.6226 - accuracy: 0.7467 - val_loss: 1.3417 - val_accuracy: 0.4694 - lr: 8.1873e-04\n",
      "Epoch 141/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6502 - accuracy: 0.7412\n",
      "Epoch 141: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 78ms/step - loss: 0.6502 - accuracy: 0.7412 - val_loss: 1.1539 - val_accuracy: 0.5323 - lr: 8.1873e-04\n",
      "Epoch 142/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6339 - accuracy: 0.7420\n",
      "Epoch 142: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.6339 - accuracy: 0.7420 - val_loss: 1.9075 - val_accuracy: 0.4150 - lr: 8.1873e-04\n",
      "Epoch 143/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6080 - accuracy: 0.7433\n",
      "Epoch 143: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.6080 - accuracy: 0.7433 - val_loss: 1.1682 - val_accuracy: 0.5374 - lr: 8.1873e-04\n",
      "Epoch 144/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5932 - accuracy: 0.7578\n",
      "Epoch 144: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.5932 - accuracy: 0.7578 - val_loss: 1.4130 - val_accuracy: 0.5459 - lr: 8.1873e-04\n",
      "Epoch 145/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6091 - accuracy: 0.7501\n",
      "Epoch 145: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.6091 - accuracy: 0.7501 - val_loss: 1.4227 - val_accuracy: 0.5102 - lr: 8.1873e-04\n",
      "Epoch 146/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5552 - accuracy: 0.7705\n",
      "Epoch 146: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.5552 - accuracy: 0.7705 - val_loss: 1.1669 - val_accuracy: 0.5391 - lr: 8.1873e-04\n",
      "Epoch 147/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6095 - accuracy: 0.7514\n",
      "Epoch 147: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.6095 - accuracy: 0.7514 - val_loss: 1.2865 - val_accuracy: 0.5136 - lr: 8.1873e-04\n",
      "Epoch 148/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6121 - accuracy: 0.7501\n",
      "Epoch 148: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.6121 - accuracy: 0.7501 - val_loss: 1.1674 - val_accuracy: 0.5612 - lr: 8.1873e-04\n",
      "Epoch 149/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6230 - accuracy: 0.7344\n",
      "Epoch 149: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.6230 - accuracy: 0.7344 - val_loss: 1.2098 - val_accuracy: 0.5289 - lr: 8.1873e-04\n",
      "Epoch 150/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6065 - accuracy: 0.7467\n",
      "Epoch 150: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.6065 - accuracy: 0.7467 - val_loss: 1.8714 - val_accuracy: 0.4303 - lr: 8.1873e-04\n",
      "Epoch 151/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6098 - accuracy: 0.7433\n",
      "Epoch 151: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.6098 - accuracy: 0.7433 - val_loss: 1.1281 - val_accuracy: 0.5765 - lr: 8.1873e-04\n",
      "Epoch 152/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5909 - accuracy: 0.7565\n",
      "Epoch 152: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.5909 - accuracy: 0.7565 - val_loss: 1.3420 - val_accuracy: 0.5340 - lr: 8.1873e-04\n",
      "Epoch 153/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6086 - accuracy: 0.7544\n",
      "Epoch 153: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.6086 - accuracy: 0.7544 - val_loss: 1.3547 - val_accuracy: 0.5289 - lr: 8.1873e-04\n",
      "Epoch 154/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6055 - accuracy: 0.7612\n",
      "Epoch 154: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.6055 - accuracy: 0.7612 - val_loss: 2.0318 - val_accuracy: 0.4354 - lr: 8.1873e-04\n",
      "Epoch 155/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6000 - accuracy: 0.7493\n",
      "Epoch 155: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.6000 - accuracy: 0.7493 - val_loss: 1.1331 - val_accuracy: 0.5425 - lr: 8.1873e-04\n",
      "Epoch 156/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6230 - accuracy: 0.7412\n",
      "Epoch 156: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.6230 - accuracy: 0.7412 - val_loss: 1.1221 - val_accuracy: 0.5595 - lr: 8.1873e-04\n",
      "Epoch 157/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5654 - accuracy: 0.7791\n",
      "Epoch 157: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.5654 - accuracy: 0.7791 - val_loss: 1.4855 - val_accuracy: 0.4881 - lr: 8.1873e-04\n",
      "Epoch 158/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5886 - accuracy: 0.7646\n",
      "Epoch 158: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.5886 - accuracy: 0.7646 - val_loss: 1.1864 - val_accuracy: 0.5646 - lr: 8.1873e-04\n",
      "Epoch 159/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5905 - accuracy: 0.7544\n",
      "Epoch 159: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.5905 - accuracy: 0.7544 - val_loss: 1.8528 - val_accuracy: 0.4813 - lr: 8.1873e-04\n",
      "Epoch 160/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5616 - accuracy: 0.7765\n",
      "Epoch 160: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.5616 - accuracy: 0.7765 - val_loss: 1.2491 - val_accuracy: 0.5595 - lr: 8.1873e-04\n",
      "Epoch 161/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5783 - accuracy: 0.7701\n",
      "Epoch 161: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.5783 - accuracy: 0.7701 - val_loss: 1.3466 - val_accuracy: 0.5340 - lr: 8.1873e-04\n",
      "Epoch 162/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5672 - accuracy: 0.7659\n",
      "Epoch 162: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.5672 - accuracy: 0.7659 - val_loss: 1.4635 - val_accuracy: 0.4779 - lr: 8.1873e-04\n",
      "Epoch 163/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5845 - accuracy: 0.7595\n",
      "Epoch 163: val_accuracy did not improve from 0.58333\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.5845 - accuracy: 0.7595 - val_loss: 1.2385 - val_accuracy: 0.5527 - lr: 8.1873e-04\n",
      "Epoch 164/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5646 - accuracy: 0.7786\n",
      "Epoch 164: val_accuracy improved from 0.58333 to 0.59694, saving model to ./models/ResNet50_Dense_add.hdf5\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.5646 - accuracy: 0.7786 - val_loss: 1.0662 - val_accuracy: 0.5969 - lr: 8.1873e-04\n",
      "Epoch 165/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5851 - accuracy: 0.7735\n",
      "Epoch 165: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.5851 - accuracy: 0.7735 - val_loss: 1.3488 - val_accuracy: 0.5306 - lr: 8.1873e-04\n",
      "Epoch 166/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5695 - accuracy: 0.7629\n",
      "Epoch 166: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.5695 - accuracy: 0.7629 - val_loss: 2.4472 - val_accuracy: 0.4133 - lr: 8.1873e-04\n",
      "Epoch 167/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5532 - accuracy: 0.7829\n",
      "Epoch 167: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.5532 - accuracy: 0.7829 - val_loss: 1.3522 - val_accuracy: 0.5612 - lr: 8.1873e-04\n",
      "Epoch 168/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5697 - accuracy: 0.7701\n",
      "Epoch 168: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.5697 - accuracy: 0.7701 - val_loss: 1.9747 - val_accuracy: 0.5153 - lr: 8.1873e-04\n",
      "Epoch 169/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5922 - accuracy: 0.7620\n",
      "Epoch 169: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.5922 - accuracy: 0.7620 - val_loss: 1.2385 - val_accuracy: 0.5544 - lr: 8.1873e-04\n",
      "Epoch 170/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5534 - accuracy: 0.7705\n",
      "Epoch 170: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.5534 - accuracy: 0.7705 - val_loss: 1.4079 - val_accuracy: 0.5374 - lr: 8.1873e-04\n",
      "Epoch 171/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5861 - accuracy: 0.7595\n",
      "Epoch 171: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 83ms/step - loss: 0.5861 - accuracy: 0.7595 - val_loss: 1.3939 - val_accuracy: 0.5187 - lr: 8.1873e-04\n",
      "Epoch 172/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5363 - accuracy: 0.7880\n",
      "Epoch 172: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.5363 - accuracy: 0.7880 - val_loss: 1.3050 - val_accuracy: 0.5323 - lr: 8.1873e-04\n",
      "Epoch 173/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5541 - accuracy: 0.7833\n",
      "Epoch 173: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 83ms/step - loss: 0.5541 - accuracy: 0.7833 - val_loss: 1.4043 - val_accuracy: 0.4898 - lr: 8.1873e-04\n",
      "Epoch 174/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5719 - accuracy: 0.7718\n",
      "Epoch 174: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 83ms/step - loss: 0.5719 - accuracy: 0.7718 - val_loss: 1.5294 - val_accuracy: 0.5017 - lr: 8.1873e-04\n",
      "Epoch 175/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5602 - accuracy: 0.7752\n",
      "Epoch 175: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 13s 85ms/step - loss: 0.5602 - accuracy: 0.7752 - val_loss: 1.3569 - val_accuracy: 0.5357 - lr: 8.1873e-04\n",
      "Epoch 176/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5607 - accuracy: 0.7654\n",
      "Epoch 176: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 84ms/step - loss: 0.5607 - accuracy: 0.7654 - val_loss: 1.2249 - val_accuracy: 0.5561 - lr: 8.1873e-04\n",
      "Epoch 177/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5245 - accuracy: 0.7910\n",
      "Epoch 177: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 84ms/step - loss: 0.5245 - accuracy: 0.7910 - val_loss: 1.2961 - val_accuracy: 0.5544 - lr: 8.1873e-04\n",
      "Epoch 178/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5566 - accuracy: 0.7718\n",
      "Epoch 178: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 13s 87ms/step - loss: 0.5566 - accuracy: 0.7718 - val_loss: 1.2999 - val_accuracy: 0.5578 - lr: 8.1873e-04\n",
      "Epoch 179/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5359 - accuracy: 0.7816\n",
      "Epoch 179: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.5359 - accuracy: 0.7816 - val_loss: 1.1563 - val_accuracy: 0.5833 - lr: 8.1873e-04\n",
      "Epoch 180/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5505 - accuracy: 0.7782\n",
      "Epoch 180: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 83ms/step - loss: 0.5505 - accuracy: 0.7782 - val_loss: 2.3740 - val_accuracy: 0.4626 - lr: 8.1873e-04\n",
      "Epoch 181/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5672 - accuracy: 0.7765\n",
      "Epoch 181: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.5672 - accuracy: 0.7765 - val_loss: 1.2408 - val_accuracy: 0.5374 - lr: 8.1873e-04\n",
      "Epoch 182/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5350 - accuracy: 0.7893\n",
      "Epoch 182: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 83ms/step - loss: 0.5350 - accuracy: 0.7893 - val_loss: 1.1911 - val_accuracy: 0.5544 - lr: 8.1873e-04\n",
      "Epoch 183/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5349 - accuracy: 0.7854\n",
      "Epoch 183: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.5349 - accuracy: 0.7854 - val_loss: 1.5058 - val_accuracy: 0.4643 - lr: 8.1873e-04\n",
      "Epoch 184/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5577 - accuracy: 0.7727\n",
      "Epoch 184: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 84ms/step - loss: 0.5577 - accuracy: 0.7727 - val_loss: 1.6627 - val_accuracy: 0.4728 - lr: 8.1873e-04\n",
      "Epoch 185/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5374 - accuracy: 0.7854\n",
      "Epoch 185: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.5374 - accuracy: 0.7854 - val_loss: 1.0496 - val_accuracy: 0.5850 - lr: 8.1873e-04\n",
      "Epoch 186/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5807 - accuracy: 0.7544\n",
      "Epoch 186: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.5807 - accuracy: 0.7544 - val_loss: 1.6944 - val_accuracy: 0.4847 - lr: 8.1873e-04\n",
      "Epoch 187/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5452 - accuracy: 0.7850\n",
      "Epoch 187: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.5452 - accuracy: 0.7850 - val_loss: 1.5030 - val_accuracy: 0.5255 - lr: 8.1873e-04\n",
      "Epoch 188/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5060 - accuracy: 0.8003\n",
      "Epoch 188: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.5060 - accuracy: 0.8003 - val_loss: 1.4371 - val_accuracy: 0.5408 - lr: 8.1873e-04\n",
      "Epoch 189/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5210 - accuracy: 0.7914\n",
      "Epoch 189: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.5210 - accuracy: 0.7914 - val_loss: 1.6576 - val_accuracy: 0.5374 - lr: 8.1873e-04\n",
      "Epoch 190/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5482 - accuracy: 0.7795\n",
      "Epoch 190: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.5482 - accuracy: 0.7795 - val_loss: 1.8713 - val_accuracy: 0.4524 - lr: 8.1873e-04\n",
      "Epoch 191/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5542 - accuracy: 0.7791\n",
      "Epoch 191: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.5542 - accuracy: 0.7791 - val_loss: 1.4324 - val_accuracy: 0.4932 - lr: 8.1873e-04\n",
      "Epoch 192/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5212 - accuracy: 0.7820\n",
      "Epoch 192: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.5212 - accuracy: 0.7820 - val_loss: 1.2642 - val_accuracy: 0.5748 - lr: 8.1873e-04\n",
      "Epoch 193/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5109 - accuracy: 0.7940\n",
      "Epoch 193: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.5109 - accuracy: 0.7940 - val_loss: 1.4089 - val_accuracy: 0.5714 - lr: 8.1873e-04\n",
      "Epoch 194/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5314 - accuracy: 0.7820\n",
      "Epoch 194: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.5314 - accuracy: 0.7820 - val_loss: 1.4898 - val_accuracy: 0.5289 - lr: 8.1873e-04\n",
      "Epoch 195/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5278 - accuracy: 0.7710\n",
      "Epoch 195: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.5278 - accuracy: 0.7710 - val_loss: 1.3166 - val_accuracy: 0.5510 - lr: 8.1873e-04\n",
      "Epoch 196/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5034 - accuracy: 0.7880\n",
      "Epoch 196: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.5034 - accuracy: 0.7880 - val_loss: 1.2610 - val_accuracy: 0.5816 - lr: 8.1873e-04\n",
      "Epoch 197/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5297 - accuracy: 0.7863\n",
      "Epoch 197: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.5297 - accuracy: 0.7863 - val_loss: 3.5899 - val_accuracy: 0.4303 - lr: 8.1873e-04\n",
      "Epoch 198/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5240 - accuracy: 0.8042\n",
      "Epoch 198: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.5240 - accuracy: 0.8042 - val_loss: 1.3662 - val_accuracy: 0.5731 - lr: 8.1873e-04\n",
      "Epoch 199/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5115 - accuracy: 0.7935\n",
      "Epoch 199: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 13s 88ms/step - loss: 0.5115 - accuracy: 0.7935 - val_loss: 1.4821 - val_accuracy: 0.5034 - lr: 8.1873e-04\n",
      "Epoch 200/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4989 - accuracy: 0.7986\n",
      "Epoch 200: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 85ms/step - loss: 0.4989 - accuracy: 0.7986 - val_loss: 1.5183 - val_accuracy: 0.5136 - lr: 8.1873e-04\n",
      "Epoch 201/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5386 - accuracy: 0.7778\n",
      "Epoch 201: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 13s 85ms/step - loss: 0.5386 - accuracy: 0.7778 - val_loss: 1.5888 - val_accuracy: 0.4898 - lr: 7.4082e-04\n",
      "Epoch 202/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4984 - accuracy: 0.7965\n",
      "Epoch 202: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 14s 96ms/step - loss: 0.4984 - accuracy: 0.7965 - val_loss: 1.2300 - val_accuracy: 0.5935 - lr: 7.4082e-04\n",
      "Epoch 203/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5063 - accuracy: 0.8025\n",
      "Epoch 203: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 15s 104ms/step - loss: 0.5063 - accuracy: 0.8025 - val_loss: 1.4128 - val_accuracy: 0.5289 - lr: 7.4082e-04\n",
      "Epoch 204/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5232 - accuracy: 0.7876\n",
      "Epoch 204: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 17s 114ms/step - loss: 0.5232 - accuracy: 0.7876 - val_loss: 1.3289 - val_accuracy: 0.5136 - lr: 7.4082e-04\n",
      "Epoch 205/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5007 - accuracy: 0.7999\n",
      "Epoch 205: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 14s 93ms/step - loss: 0.5007 - accuracy: 0.7999 - val_loss: 1.1829 - val_accuracy: 0.5833 - lr: 7.4082e-04\n",
      "Epoch 206/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4918 - accuracy: 0.8033\n",
      "Epoch 206: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 15s 100ms/step - loss: 0.4918 - accuracy: 0.8033 - val_loss: 1.1084 - val_accuracy: 0.5952 - lr: 7.4082e-04\n",
      "Epoch 207/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4992 - accuracy: 0.8054\n",
      "Epoch 207: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 13s 87ms/step - loss: 0.4992 - accuracy: 0.8054 - val_loss: 1.3385 - val_accuracy: 0.5799 - lr: 7.4082e-04\n",
      "Epoch 208/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4884 - accuracy: 0.8054\n",
      "Epoch 208: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 13s 91ms/step - loss: 0.4884 - accuracy: 0.8054 - val_loss: 2.0528 - val_accuracy: 0.5153 - lr: 7.4082e-04\n",
      "Epoch 209/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5049 - accuracy: 0.8093\n",
      "Epoch 209: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 83ms/step - loss: 0.5049 - accuracy: 0.8093 - val_loss: 1.9600 - val_accuracy: 0.4558 - lr: 7.4082e-04\n",
      "Epoch 210/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5009 - accuracy: 0.8072\n",
      "Epoch 210: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 84ms/step - loss: 0.5009 - accuracy: 0.8072 - val_loss: 1.4657 - val_accuracy: 0.5578 - lr: 7.4082e-04\n",
      "Epoch 211/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4964 - accuracy: 0.8072\n",
      "Epoch 211: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.4964 - accuracy: 0.8072 - val_loss: 1.3276 - val_accuracy: 0.5782 - lr: 7.4082e-04\n",
      "Epoch 212/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5223 - accuracy: 0.7931\n",
      "Epoch 212: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.5223 - accuracy: 0.7931 - val_loss: 1.8584 - val_accuracy: 0.5238 - lr: 7.4082e-04\n",
      "Epoch 213/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5140 - accuracy: 0.7918\n",
      "Epoch 213: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.5140 - accuracy: 0.7918 - val_loss: 1.4965 - val_accuracy: 0.5340 - lr: 7.4082e-04\n",
      "Epoch 214/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4997 - accuracy: 0.7944\n",
      "Epoch 214: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4997 - accuracy: 0.7944 - val_loss: 1.3594 - val_accuracy: 0.5510 - lr: 7.4082e-04\n",
      "Epoch 215/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4952 - accuracy: 0.7969\n",
      "Epoch 215: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4952 - accuracy: 0.7969 - val_loss: 1.3507 - val_accuracy: 0.5561 - lr: 7.4082e-04\n",
      "Epoch 216/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4855 - accuracy: 0.7884\n",
      "Epoch 216: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4855 - accuracy: 0.7884 - val_loss: 1.2678 - val_accuracy: 0.5816 - lr: 7.4082e-04\n",
      "Epoch 217/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4749 - accuracy: 0.8046\n",
      "Epoch 217: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4749 - accuracy: 0.8046 - val_loss: 1.2179 - val_accuracy: 0.5867 - lr: 7.4082e-04\n",
      "Epoch 218/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4796 - accuracy: 0.8029\n",
      "Epoch 218: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4796 - accuracy: 0.8029 - val_loss: 1.2944 - val_accuracy: 0.5833 - lr: 7.4082e-04\n",
      "Epoch 219/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4843 - accuracy: 0.8106\n",
      "Epoch 219: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.4843 - accuracy: 0.8106 - val_loss: 1.1917 - val_accuracy: 0.5833 - lr: 7.4082e-04\n",
      "Epoch 220/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4965 - accuracy: 0.7931\n",
      "Epoch 220: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.4965 - accuracy: 0.7931 - val_loss: 1.1833 - val_accuracy: 0.5850 - lr: 7.4082e-04\n",
      "Epoch 221/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4907 - accuracy: 0.7999\n",
      "Epoch 221: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4907 - accuracy: 0.7999 - val_loss: 1.2931 - val_accuracy: 0.5595 - lr: 7.4082e-04\n",
      "Epoch 222/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5174 - accuracy: 0.7944\n",
      "Epoch 222: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.5174 - accuracy: 0.7944 - val_loss: 1.6951 - val_accuracy: 0.4643 - lr: 7.4082e-04\n",
      "Epoch 223/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4609 - accuracy: 0.8174\n",
      "Epoch 223: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4609 - accuracy: 0.8174 - val_loss: 1.3426 - val_accuracy: 0.5850 - lr: 7.4082e-04\n",
      "Epoch 224/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4873 - accuracy: 0.8059\n",
      "Epoch 224: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.4873 - accuracy: 0.8059 - val_loss: 2.0155 - val_accuracy: 0.5000 - lr: 7.4082e-04\n",
      "Epoch 225/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5096 - accuracy: 0.7944\n",
      "Epoch 225: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.5096 - accuracy: 0.7944 - val_loss: 1.5454 - val_accuracy: 0.5527 - lr: 7.4082e-04\n",
      "Epoch 226/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4932 - accuracy: 0.8054\n",
      "Epoch 226: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.4932 - accuracy: 0.8054 - val_loss: 2.1284 - val_accuracy: 0.4218 - lr: 7.4082e-04\n",
      "Epoch 227/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4839 - accuracy: 0.8042\n",
      "Epoch 227: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 78ms/step - loss: 0.4839 - accuracy: 0.8042 - val_loss: 2.3884 - val_accuracy: 0.3997 - lr: 7.4082e-04\n",
      "Epoch 228/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4684 - accuracy: 0.8152\n",
      "Epoch 228: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.4684 - accuracy: 0.8152 - val_loss: 1.4539 - val_accuracy: 0.5357 - lr: 7.4082e-04\n",
      "Epoch 229/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4846 - accuracy: 0.8093\n",
      "Epoch 229: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4846 - accuracy: 0.8093 - val_loss: 1.7204 - val_accuracy: 0.5017 - lr: 7.4082e-04\n",
      "Epoch 230/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4874 - accuracy: 0.7969\n",
      "Epoch 230: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.4874 - accuracy: 0.7969 - val_loss: 1.3461 - val_accuracy: 0.5799 - lr: 7.4082e-04\n",
      "Epoch 231/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4875 - accuracy: 0.8046\n",
      "Epoch 231: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.4875 - accuracy: 0.8046 - val_loss: 1.6217 - val_accuracy: 0.5034 - lr: 7.4082e-04\n",
      "Epoch 232/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4738 - accuracy: 0.8131\n",
      "Epoch 232: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.4738 - accuracy: 0.8131 - val_loss: 1.3137 - val_accuracy: 0.5748 - lr: 7.4082e-04\n",
      "Epoch 233/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4730 - accuracy: 0.8148\n",
      "Epoch 233: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.4730 - accuracy: 0.8148 - val_loss: 1.8777 - val_accuracy: 0.4286 - lr: 7.4082e-04\n",
      "Epoch 234/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4712 - accuracy: 0.8059\n",
      "Epoch 234: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.4712 - accuracy: 0.8059 - val_loss: 1.2609 - val_accuracy: 0.5663 - lr: 7.4082e-04\n",
      "Epoch 235/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4373 - accuracy: 0.8233\n",
      "Epoch 235: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.4373 - accuracy: 0.8233 - val_loss: 1.5729 - val_accuracy: 0.5595 - lr: 7.4082e-04\n",
      "Epoch 236/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4869 - accuracy: 0.8008\n",
      "Epoch 236: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 83ms/step - loss: 0.4869 - accuracy: 0.8008 - val_loss: 1.6754 - val_accuracy: 0.5714 - lr: 7.4082e-04\n",
      "Epoch 237/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4637 - accuracy: 0.8067\n",
      "Epoch 237: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.4637 - accuracy: 0.8067 - val_loss: 1.3901 - val_accuracy: 0.5714 - lr: 7.4082e-04\n",
      "Epoch 238/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4697 - accuracy: 0.8182\n",
      "Epoch 238: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.4697 - accuracy: 0.8182 - val_loss: 1.4857 - val_accuracy: 0.5306 - lr: 7.4082e-04\n",
      "Epoch 239/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4919 - accuracy: 0.8003\n",
      "Epoch 239: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4919 - accuracy: 0.8003 - val_loss: 1.2190 - val_accuracy: 0.5816 - lr: 7.4082e-04\n",
      "Epoch 240/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4880 - accuracy: 0.8033\n",
      "Epoch 240: val_accuracy did not improve from 0.59694\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.4880 - accuracy: 0.8033 - val_loss: 1.5119 - val_accuracy: 0.5527 - lr: 7.4082e-04\n",
      "Epoch 241/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4839 - accuracy: 0.8093\n",
      "Epoch 241: val_accuracy improved from 0.59694 to 0.60714, saving model to ./models/ResNet50_Dense_add.hdf5\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4839 - accuracy: 0.8093 - val_loss: 1.1992 - val_accuracy: 0.6071 - lr: 7.4082e-04\n",
      "Epoch 242/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4601 - accuracy: 0.8089\n",
      "Epoch 242: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.4601 - accuracy: 0.8089 - val_loss: 1.3245 - val_accuracy: 0.5918 - lr: 7.4082e-04\n",
      "Epoch 243/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4457 - accuracy: 0.8182\n",
      "Epoch 243: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.4457 - accuracy: 0.8182 - val_loss: 1.3413 - val_accuracy: 0.5935 - lr: 7.4082e-04\n",
      "Epoch 244/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4471 - accuracy: 0.8186\n",
      "Epoch 244: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.4471 - accuracy: 0.8186 - val_loss: 1.4225 - val_accuracy: 0.5799 - lr: 7.4082e-04\n",
      "Epoch 245/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4537 - accuracy: 0.8275\n",
      "Epoch 245: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.4559 - accuracy: 0.8267 - val_loss: 1.4154 - val_accuracy: 0.5544 - lr: 7.4082e-04\n",
      "Epoch 246/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4829 - accuracy: 0.8063\n",
      "Epoch 246: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.4829 - accuracy: 0.8063 - val_loss: 1.2911 - val_accuracy: 0.5765 - lr: 7.4082e-04\n",
      "Epoch 247/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4652 - accuracy: 0.8140\n",
      "Epoch 247: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4652 - accuracy: 0.8140 - val_loss: 1.3255 - val_accuracy: 0.5799 - lr: 7.4082e-04\n",
      "Epoch 248/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4519 - accuracy: 0.8127\n",
      "Epoch 248: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.4519 - accuracy: 0.8127 - val_loss: 1.2283 - val_accuracy: 0.5986 - lr: 7.4082e-04\n",
      "Epoch 249/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4414 - accuracy: 0.8238\n",
      "Epoch 249: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4414 - accuracy: 0.8238 - val_loss: 1.4370 - val_accuracy: 0.5595 - lr: 7.4082e-04\n",
      "Epoch 250/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4599 - accuracy: 0.8114\n",
      "Epoch 250: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4599 - accuracy: 0.8114 - val_loss: 1.4797 - val_accuracy: 0.5561 - lr: 7.4082e-04\n",
      "Epoch 251/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4630 - accuracy: 0.8110\n",
      "Epoch 251: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4630 - accuracy: 0.8110 - val_loss: 1.3405 - val_accuracy: 0.6054 - lr: 7.4082e-04\n",
      "Epoch 252/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4735 - accuracy: 0.8135\n",
      "Epoch 252: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4735 - accuracy: 0.8135 - val_loss: 1.4794 - val_accuracy: 0.5833 - lr: 7.4082e-04\n",
      "Epoch 253/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4597 - accuracy: 0.8050\n",
      "Epoch 253: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.4597 - accuracy: 0.8050 - val_loss: 1.5403 - val_accuracy: 0.5561 - lr: 7.4082e-04\n",
      "Epoch 254/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4575 - accuracy: 0.8165\n",
      "Epoch 254: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4575 - accuracy: 0.8165 - val_loss: 1.2361 - val_accuracy: 0.5901 - lr: 7.4082e-04\n",
      "Epoch 255/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4475 - accuracy: 0.8140\n",
      "Epoch 255: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4475 - accuracy: 0.8140 - val_loss: 1.8914 - val_accuracy: 0.4966 - lr: 7.4082e-04\n",
      "Epoch 256/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4795 - accuracy: 0.8186\n",
      "Epoch 256: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4795 - accuracy: 0.8186 - val_loss: 1.3927 - val_accuracy: 0.5748 - lr: 7.4082e-04\n",
      "Epoch 257/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4626 - accuracy: 0.8140\n",
      "Epoch 257: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4626 - accuracy: 0.8140 - val_loss: 1.3537 - val_accuracy: 0.5816 - lr: 7.4082e-04\n",
      "Epoch 258/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4556 - accuracy: 0.8148\n",
      "Epoch 258: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.4556 - accuracy: 0.8148 - val_loss: 1.6333 - val_accuracy: 0.5884 - lr: 7.4082e-04\n",
      "Epoch 259/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4510 - accuracy: 0.8174\n",
      "Epoch 259: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 85ms/step - loss: 0.4510 - accuracy: 0.8174 - val_loss: 1.6271 - val_accuracy: 0.5646 - lr: 7.4082e-04\n",
      "Epoch 260/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4351 - accuracy: 0.8344\n",
      "Epoch 260: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4351 - accuracy: 0.8344 - val_loss: 1.7751 - val_accuracy: 0.5170 - lr: 7.4082e-04\n",
      "Epoch 261/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4584 - accuracy: 0.8221\n",
      "Epoch 261: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4584 - accuracy: 0.8221 - val_loss: 1.9685 - val_accuracy: 0.5170 - lr: 7.4082e-04\n",
      "Epoch 262/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4459 - accuracy: 0.8221\n",
      "Epoch 262: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4459 - accuracy: 0.8221 - val_loss: 1.2000 - val_accuracy: 0.5833 - lr: 7.4082e-04\n",
      "Epoch 263/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4713 - accuracy: 0.8148\n",
      "Epoch 263: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.4713 - accuracy: 0.8148 - val_loss: 1.7751 - val_accuracy: 0.5153 - lr: 7.4082e-04\n",
      "Epoch 264/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4451 - accuracy: 0.8314\n",
      "Epoch 264: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4451 - accuracy: 0.8314 - val_loss: 1.2531 - val_accuracy: 0.5816 - lr: 7.4082e-04\n",
      "Epoch 265/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4447 - accuracy: 0.8284\n",
      "Epoch 265: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 83ms/step - loss: 0.4447 - accuracy: 0.8284 - val_loss: 1.5000 - val_accuracy: 0.5986 - lr: 7.4082e-04\n",
      "Epoch 266/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4458 - accuracy: 0.8238\n",
      "Epoch 266: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4458 - accuracy: 0.8238 - val_loss: 1.4164 - val_accuracy: 0.5850 - lr: 7.4082e-04\n",
      "Epoch 267/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4651 - accuracy: 0.8203\n",
      "Epoch 267: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.4651 - accuracy: 0.8203 - val_loss: 1.4064 - val_accuracy: 0.5578 - lr: 7.4082e-04\n",
      "Epoch 268/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4521 - accuracy: 0.8246\n",
      "Epoch 268: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.4521 - accuracy: 0.8246 - val_loss: 1.4931 - val_accuracy: 0.5646 - lr: 7.4082e-04\n",
      "Epoch 269/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4423 - accuracy: 0.8089\n",
      "Epoch 269: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.4423 - accuracy: 0.8089 - val_loss: 1.2125 - val_accuracy: 0.5782 - lr: 7.4082e-04\n",
      "Epoch 270/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4572 - accuracy: 0.8216\n",
      "Epoch 270: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4572 - accuracy: 0.8216 - val_loss: 1.5712 - val_accuracy: 0.5680 - lr: 7.4082e-04\n",
      "Epoch 271/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4405 - accuracy: 0.8238\n",
      "Epoch 271: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4405 - accuracy: 0.8238 - val_loss: 1.6275 - val_accuracy: 0.5595 - lr: 7.4082e-04\n",
      "Epoch 272/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4537 - accuracy: 0.8301\n",
      "Epoch 272: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4537 - accuracy: 0.8301 - val_loss: 1.6096 - val_accuracy: 0.4949 - lr: 7.4082e-04\n",
      "Epoch 273/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4255 - accuracy: 0.8318\n",
      "Epoch 273: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.4255 - accuracy: 0.8318 - val_loss: 1.4537 - val_accuracy: 0.5697 - lr: 7.4082e-04\n",
      "Epoch 274/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4622 - accuracy: 0.8148\n",
      "Epoch 274: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 84ms/step - loss: 0.4622 - accuracy: 0.8148 - val_loss: 1.5892 - val_accuracy: 0.5442 - lr: 7.4082e-04\n",
      "Epoch 275/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4409 - accuracy: 0.8203\n",
      "Epoch 275: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 84ms/step - loss: 0.4409 - accuracy: 0.8203 - val_loss: 1.4418 - val_accuracy: 0.5612 - lr: 7.4082e-04\n",
      "Epoch 276/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4373 - accuracy: 0.8250\n",
      "Epoch 276: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 84ms/step - loss: 0.4373 - accuracy: 0.8250 - val_loss: 1.3784 - val_accuracy: 0.5731 - lr: 7.4082e-04\n",
      "Epoch 277/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4801 - accuracy: 0.8059\n",
      "Epoch 277: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 84ms/step - loss: 0.4801 - accuracy: 0.8059 - val_loss: 1.3775 - val_accuracy: 0.5544 - lr: 7.4082e-04\n",
      "Epoch 278/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4393 - accuracy: 0.8242\n",
      "Epoch 278: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.4393 - accuracy: 0.8242 - val_loss: 1.4471 - val_accuracy: 0.5867 - lr: 7.4082e-04\n",
      "Epoch 279/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4461 - accuracy: 0.8216\n",
      "Epoch 279: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.4461 - accuracy: 0.8216 - val_loss: 1.8033 - val_accuracy: 0.5391 - lr: 7.4082e-04\n",
      "Epoch 280/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4539 - accuracy: 0.8191\n",
      "Epoch 280: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4539 - accuracy: 0.8191 - val_loss: 2.6940 - val_accuracy: 0.4065 - lr: 7.4082e-04\n",
      "Epoch 281/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4355 - accuracy: 0.8267\n",
      "Epoch 281: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4355 - accuracy: 0.8267 - val_loss: 1.6964 - val_accuracy: 0.5391 - lr: 7.4082e-04\n",
      "Epoch 282/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4279 - accuracy: 0.8310\n",
      "Epoch 282: val_accuracy did not improve from 0.60714\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4279 - accuracy: 0.8310 - val_loss: 1.5348 - val_accuracy: 0.5833 - lr: 7.4082e-04\n",
      "Epoch 283/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4220 - accuracy: 0.8344\n",
      "Epoch 283: val_accuracy improved from 0.60714 to 0.61565, saving model to ./models/ResNet50_Dense_add.hdf5\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4220 - accuracy: 0.8344 - val_loss: 1.3109 - val_accuracy: 0.6156 - lr: 7.4082e-04\n",
      "Epoch 284/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4364 - accuracy: 0.8314\n",
      "Epoch 284: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4364 - accuracy: 0.8314 - val_loss: 1.6529 - val_accuracy: 0.5442 - lr: 7.4082e-04\n",
      "Epoch 285/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4513 - accuracy: 0.8233\n",
      "Epoch 285: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4513 - accuracy: 0.8233 - val_loss: 1.3579 - val_accuracy: 0.5918 - lr: 7.4082e-04\n",
      "Epoch 286/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4088 - accuracy: 0.8399\n",
      "Epoch 286: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4088 - accuracy: 0.8399 - val_loss: 1.8118 - val_accuracy: 0.4983 - lr: 7.4082e-04\n",
      "Epoch 287/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4609 - accuracy: 0.8174\n",
      "Epoch 287: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.4609 - accuracy: 0.8174 - val_loss: 1.2627 - val_accuracy: 0.6088 - lr: 7.4082e-04\n",
      "Epoch 288/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4324 - accuracy: 0.8344\n",
      "Epoch 288: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 12s 83ms/step - loss: 0.4324 - accuracy: 0.8344 - val_loss: 1.3225 - val_accuracy: 0.5714 - lr: 7.4082e-04\n",
      "Epoch 289/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4319 - accuracy: 0.8246\n",
      "Epoch 289: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.4319 - accuracy: 0.8246 - val_loss: 1.5887 - val_accuracy: 0.5697 - lr: 7.4082e-04\n",
      "Epoch 290/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4369 - accuracy: 0.8301\n",
      "Epoch 290: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.4369 - accuracy: 0.8301 - val_loss: 1.3292 - val_accuracy: 0.6054 - lr: 7.4082e-04\n",
      "Epoch 291/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4377 - accuracy: 0.8289\n",
      "Epoch 291: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.4377 - accuracy: 0.8289 - val_loss: 1.4001 - val_accuracy: 0.5782 - lr: 7.4082e-04\n",
      "Epoch 292/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4400 - accuracy: 0.8169\n",
      "Epoch 292: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.4400 - accuracy: 0.8169 - val_loss: 1.4663 - val_accuracy: 0.5646 - lr: 7.4082e-04\n",
      "Epoch 293/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4274 - accuracy: 0.8352\n",
      "Epoch 293: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.4274 - accuracy: 0.8352 - val_loss: 1.3630 - val_accuracy: 0.5850 - lr: 7.4082e-04\n",
      "Epoch 294/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4186 - accuracy: 0.8327\n",
      "Epoch 294: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.4186 - accuracy: 0.8327 - val_loss: 1.3469 - val_accuracy: 0.5595 - lr: 7.4082e-04\n",
      "Epoch 295/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4733 - accuracy: 0.8140\n",
      "Epoch 295: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.4733 - accuracy: 0.8140 - val_loss: 1.3040 - val_accuracy: 0.5612 - lr: 7.4082e-04\n",
      "Epoch 296/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4225 - accuracy: 0.8318\n",
      "Epoch 296: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.4225 - accuracy: 0.8318 - val_loss: 1.4556 - val_accuracy: 0.5935 - lr: 7.4082e-04\n",
      "Epoch 297/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4336 - accuracy: 0.8165\n",
      "Epoch 297: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.4336 - accuracy: 0.8165 - val_loss: 2.1363 - val_accuracy: 0.4677 - lr: 7.4082e-04\n",
      "Epoch 298/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4207 - accuracy: 0.8306\n",
      "Epoch 298: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.4207 - accuracy: 0.8306 - val_loss: 1.4280 - val_accuracy: 0.5867 - lr: 7.4082e-04\n",
      "Epoch 299/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4252 - accuracy: 0.8361\n",
      "Epoch 299: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4252 - accuracy: 0.8361 - val_loss: 1.5886 - val_accuracy: 0.5714 - lr: 7.4082e-04\n",
      "Epoch 300/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4448 - accuracy: 0.8369\n",
      "Epoch 300: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.4437 - accuracy: 0.8374 - val_loss: 1.4137 - val_accuracy: 0.5867 - lr: 7.4082e-04\n",
      "Epoch 301/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4460 - accuracy: 0.8181\n",
      "Epoch 301: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 10s 71ms/step - loss: 0.4467 - accuracy: 0.8182 - val_loss: 1.4431 - val_accuracy: 0.5646 - lr: 6.7032e-04\n",
      "Epoch 302/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4070 - accuracy: 0.8395\n",
      "Epoch 302: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.4060 - accuracy: 0.8404 - val_loss: 1.6522 - val_accuracy: 0.5425 - lr: 6.7032e-04\n",
      "Epoch 303/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4253 - accuracy: 0.8326\n",
      "Epoch 303: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.4242 - accuracy: 0.8335 - val_loss: 1.4940 - val_accuracy: 0.5850 - lr: 6.7032e-04\n",
      "Epoch 304/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4083 - accuracy: 0.8416\n",
      "Epoch 304: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.4081 - accuracy: 0.8416 - val_loss: 1.3386 - val_accuracy: 0.5782 - lr: 6.7032e-04\n",
      "Epoch 305/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4093 - accuracy: 0.8309\n",
      "Epoch 305: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.4075 - accuracy: 0.8318 - val_loss: 1.4999 - val_accuracy: 0.6105 - lr: 6.7032e-04\n",
      "Epoch 306/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4053 - accuracy: 0.8301\n",
      "Epoch 306: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.4049 - accuracy: 0.8297 - val_loss: 1.3846 - val_accuracy: 0.5918 - lr: 6.7032e-04\n",
      "Epoch 307/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3904 - accuracy: 0.8574\n",
      "Epoch 307: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.3897 - accuracy: 0.8578 - val_loss: 1.4914 - val_accuracy: 0.5952 - lr: 6.7032e-04\n",
      "Epoch 308/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3872 - accuracy: 0.8467\n",
      "Epoch 308: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 10s 67ms/step - loss: 0.3879 - accuracy: 0.8463 - val_loss: 1.4378 - val_accuracy: 0.6071 - lr: 6.7032e-04\n",
      "Epoch 309/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4105 - accuracy: 0.8356\n",
      "Epoch 309: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 10s 66ms/step - loss: 0.4101 - accuracy: 0.8352 - val_loss: 1.7374 - val_accuracy: 0.5051 - lr: 6.7032e-04\n",
      "Epoch 310/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4564 - accuracy: 0.8253\n",
      "Epoch 310: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 10s 68ms/step - loss: 0.4559 - accuracy: 0.8255 - val_loss: 2.3278 - val_accuracy: 0.4643 - lr: 6.7032e-04\n",
      "Epoch 311/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4381 - accuracy: 0.8241\n",
      "Epoch 311: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 10s 67ms/step - loss: 0.4379 - accuracy: 0.8238 - val_loss: 1.6504 - val_accuracy: 0.5680 - lr: 6.7032e-04\n",
      "Epoch 312/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3961 - accuracy: 0.8433\n",
      "Epoch 312: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 10s 67ms/step - loss: 0.3962 - accuracy: 0.8433 - val_loss: 1.4941 - val_accuracy: 0.5884 - lr: 6.7032e-04\n",
      "Epoch 313/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4043 - accuracy: 0.8412\n",
      "Epoch 313: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 10s 68ms/step - loss: 0.4037 - accuracy: 0.8416 - val_loss: 1.5775 - val_accuracy: 0.5884 - lr: 6.7032e-04\n",
      "Epoch 314/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4093 - accuracy: 0.8335\n",
      "Epoch 314: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 10s 70ms/step - loss: 0.4080 - accuracy: 0.8340 - val_loss: 1.6023 - val_accuracy: 0.5731 - lr: 6.7032e-04\n",
      "Epoch 315/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4124 - accuracy: 0.8378\n",
      "Epoch 315: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.4140 - accuracy: 0.8374 - val_loss: 1.7198 - val_accuracy: 0.5323 - lr: 6.7032e-04\n",
      "Epoch 316/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4032 - accuracy: 0.8365\n",
      "Epoch 316: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.4021 - accuracy: 0.8370 - val_loss: 1.5754 - val_accuracy: 0.5969 - lr: 6.7032e-04\n",
      "Epoch 317/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4228 - accuracy: 0.8283\n",
      "Epoch 317: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.4260 - accuracy: 0.8263 - val_loss: 1.3994 - val_accuracy: 0.5969 - lr: 6.7032e-04\n",
      "Epoch 318/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4423 - accuracy: 0.8352\n",
      "Epoch 318: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.4438 - accuracy: 0.8344 - val_loss: 1.4158 - val_accuracy: 0.6105 - lr: 6.7032e-04\n",
      "Epoch 319/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3987 - accuracy: 0.8527\n",
      "Epoch 319: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.3984 - accuracy: 0.8527 - val_loss: 1.5020 - val_accuracy: 0.6071 - lr: 6.7032e-04\n",
      "Epoch 320/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4113 - accuracy: 0.8296\n",
      "Epoch 320: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.4109 - accuracy: 0.8297 - val_loss: 1.4375 - val_accuracy: 0.5748 - lr: 6.7032e-04\n",
      "Epoch 321/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4357 - accuracy: 0.8211\n",
      "Epoch 321: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.4356 - accuracy: 0.8216 - val_loss: 1.8010 - val_accuracy: 0.5034 - lr: 6.7032e-04\n",
      "Epoch 322/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4140 - accuracy: 0.8438\n",
      "Epoch 322: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.4129 - accuracy: 0.8442 - val_loss: 1.3840 - val_accuracy: 0.5833 - lr: 6.7032e-04\n",
      "Epoch 323/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3669 - accuracy: 0.8527\n",
      "Epoch 323: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.3661 - accuracy: 0.8536 - val_loss: 1.4952 - val_accuracy: 0.5731 - lr: 6.7032e-04\n",
      "Epoch 324/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4049 - accuracy: 0.8446\n",
      "Epoch 324: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.4038 - accuracy: 0.8446 - val_loss: 1.6310 - val_accuracy: 0.5697 - lr: 6.7032e-04\n",
      "Epoch 325/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3920 - accuracy: 0.8369\n",
      "Epoch 325: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.3907 - accuracy: 0.8378 - val_loss: 1.7614 - val_accuracy: 0.5442 - lr: 6.7032e-04\n",
      "Epoch 326/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4174 - accuracy: 0.8283\n",
      "Epoch 326: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.4170 - accuracy: 0.8284 - val_loss: 1.8706 - val_accuracy: 0.5306 - lr: 6.7032e-04\n",
      "Epoch 327/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4242 - accuracy: 0.8382\n",
      "Epoch 327: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.4242 - accuracy: 0.8382 - val_loss: 1.3123 - val_accuracy: 0.6122 - lr: 6.7032e-04\n",
      "Epoch 328/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3984 - accuracy: 0.8459\n",
      "Epoch 328: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.3981 - accuracy: 0.8459 - val_loss: 1.5436 - val_accuracy: 0.5629 - lr: 6.7032e-04\n",
      "Epoch 329/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3847 - accuracy: 0.8489\n",
      "Epoch 329: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.3837 - accuracy: 0.8493 - val_loss: 1.7710 - val_accuracy: 0.5306 - lr: 6.7032e-04\n",
      "Epoch 330/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3771 - accuracy: 0.8553\n",
      "Epoch 330: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.3770 - accuracy: 0.8557 - val_loss: 1.4531 - val_accuracy: 0.6088 - lr: 6.7032e-04\n",
      "Epoch 331/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3686 - accuracy: 0.8566\n",
      "Epoch 331: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 72ms/step - loss: 0.3686 - accuracy: 0.8561 - val_loss: 1.8003 - val_accuracy: 0.5357 - lr: 6.7032e-04\n",
      "Epoch 332/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4185 - accuracy: 0.8463\n",
      "Epoch 332: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.4189 - accuracy: 0.8463 - val_loss: 1.6044 - val_accuracy: 0.5867 - lr: 6.7032e-04\n",
      "Epoch 333/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3710 - accuracy: 0.8476\n",
      "Epoch 333: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.3704 - accuracy: 0.8476 - val_loss: 1.5310 - val_accuracy: 0.5884 - lr: 6.7032e-04\n",
      "Epoch 334/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4077 - accuracy: 0.8450\n",
      "Epoch 334: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.4099 - accuracy: 0.8442 - val_loss: 1.4768 - val_accuracy: 0.5884 - lr: 6.7032e-04\n",
      "Epoch 335/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4203 - accuracy: 0.8382\n",
      "Epoch 335: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.4205 - accuracy: 0.8378 - val_loss: 1.5792 - val_accuracy: 0.5578 - lr: 6.7032e-04\n",
      "Epoch 336/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3885 - accuracy: 0.8480\n",
      "Epoch 336: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.3885 - accuracy: 0.8480 - val_loss: 1.4010 - val_accuracy: 0.5867 - lr: 6.7032e-04\n",
      "Epoch 337/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3911 - accuracy: 0.8450\n",
      "Epoch 337: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.3937 - accuracy: 0.8433 - val_loss: 1.5384 - val_accuracy: 0.5918 - lr: 6.7032e-04\n",
      "Epoch 338/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3768 - accuracy: 0.8545\n",
      "Epoch 338: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.3794 - accuracy: 0.8536 - val_loss: 1.5751 - val_accuracy: 0.5850 - lr: 6.7032e-04\n",
      "Epoch 339/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3895 - accuracy: 0.8395\n",
      "Epoch 339: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.3889 - accuracy: 0.8395 - val_loss: 1.6849 - val_accuracy: 0.5646 - lr: 6.7032e-04\n",
      "Epoch 340/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3704 - accuracy: 0.8523\n",
      "Epoch 340: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.3695 - accuracy: 0.8531 - val_loss: 1.4562 - val_accuracy: 0.6020 - lr: 6.7032e-04\n",
      "Epoch 341/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3920 - accuracy: 0.8566\n",
      "Epoch 341: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.3910 - accuracy: 0.8570 - val_loss: 1.6330 - val_accuracy: 0.5731 - lr: 6.7032e-04\n",
      "Epoch 342/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4104 - accuracy: 0.8429\n",
      "Epoch 342: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.4099 - accuracy: 0.8425 - val_loss: 2.7411 - val_accuracy: 0.4371 - lr: 6.7032e-04\n",
      "Epoch 343/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3599 - accuracy: 0.8626\n",
      "Epoch 343: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3592 - accuracy: 0.8629 - val_loss: 1.7546 - val_accuracy: 0.5714 - lr: 6.7032e-04\n",
      "Epoch 344/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4068 - accuracy: 0.8352\n",
      "Epoch 344: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.4068 - accuracy: 0.8352 - val_loss: 1.8597 - val_accuracy: 0.4830 - lr: 6.7032e-04\n",
      "Epoch 345/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3759 - accuracy: 0.8493\n",
      "Epoch 345: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.3759 - accuracy: 0.8493 - val_loss: 1.6257 - val_accuracy: 0.5646 - lr: 6.7032e-04\n",
      "Epoch 346/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3867 - accuracy: 0.8565\n",
      "Epoch 346: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.3867 - accuracy: 0.8565 - val_loss: 1.4045 - val_accuracy: 0.6071 - lr: 6.7032e-04\n",
      "Epoch 347/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3792 - accuracy: 0.8493\n",
      "Epoch 347: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.3809 - accuracy: 0.8484 - val_loss: 1.5624 - val_accuracy: 0.5799 - lr: 6.7032e-04\n",
      "Epoch 348/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4030 - accuracy: 0.8438\n",
      "Epoch 348: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.4030 - accuracy: 0.8438 - val_loss: 1.6634 - val_accuracy: 0.5646 - lr: 6.7032e-04\n",
      "Epoch 349/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4108 - accuracy: 0.8442\n",
      "Epoch 349: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.4132 - accuracy: 0.8433 - val_loss: 1.5104 - val_accuracy: 0.5969 - lr: 6.7032e-04\n",
      "Epoch 350/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3908 - accuracy: 0.8463\n",
      "Epoch 350: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.3910 - accuracy: 0.8463 - val_loss: 1.5582 - val_accuracy: 0.5799 - lr: 6.7032e-04\n",
      "Epoch 351/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3870 - accuracy: 0.8446\n",
      "Epoch 351: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.3870 - accuracy: 0.8446 - val_loss: 1.4640 - val_accuracy: 0.6071 - lr: 6.7032e-04\n",
      "Epoch 352/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3921 - accuracy: 0.8416\n",
      "Epoch 352: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.3927 - accuracy: 0.8412 - val_loss: 1.5079 - val_accuracy: 0.5935 - lr: 6.7032e-04\n",
      "Epoch 353/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3959 - accuracy: 0.8403\n",
      "Epoch 353: val_accuracy did not improve from 0.61565\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.3955 - accuracy: 0.8404 - val_loss: 1.9371 - val_accuracy: 0.5782 - lr: 6.7032e-04\n",
      "Epoch 354/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4006 - accuracy: 0.8510\n",
      "Epoch 354: val_accuracy improved from 0.61565 to 0.62415, saving model to ./models/ResNet50_Dense_add.hdf5\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3998 - accuracy: 0.8514 - val_loss: 1.4567 - val_accuracy: 0.6241 - lr: 6.7032e-04\n",
      "Epoch 355/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4064 - accuracy: 0.8442\n",
      "Epoch 355: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.4056 - accuracy: 0.8446 - val_loss: 1.3830 - val_accuracy: 0.5969 - lr: 6.7032e-04\n",
      "Epoch 356/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3533 - accuracy: 0.8574\n",
      "Epoch 356: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.3568 - accuracy: 0.8574 - val_loss: 1.9543 - val_accuracy: 0.4779 - lr: 6.7032e-04\n",
      "Epoch 357/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.8540\n",
      "Epoch 357: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.3831 - accuracy: 0.8527 - val_loss: 1.7806 - val_accuracy: 0.5391 - lr: 6.7032e-04\n",
      "Epoch 358/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3914 - accuracy: 0.8433\n",
      "Epoch 358: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.3935 - accuracy: 0.8425 - val_loss: 1.3697 - val_accuracy: 0.5935 - lr: 6.7032e-04\n",
      "Epoch 359/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3988 - accuracy: 0.8455\n",
      "Epoch 359: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.4010 - accuracy: 0.8442 - val_loss: 2.0346 - val_accuracy: 0.5442 - lr: 6.7032e-04\n",
      "Epoch 360/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3761 - accuracy: 0.8540\n",
      "Epoch 360: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.3761 - accuracy: 0.8540 - val_loss: 1.4853 - val_accuracy: 0.5663 - lr: 6.7032e-04\n",
      "Epoch 361/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3809 - accuracy: 0.8502\n",
      "Epoch 361: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.3799 - accuracy: 0.8506 - val_loss: 1.4030 - val_accuracy: 0.6071 - lr: 6.7032e-04\n",
      "Epoch 362/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4010 - accuracy: 0.8557\n",
      "Epoch 362: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.4010 - accuracy: 0.8561 - val_loss: 1.8215 - val_accuracy: 0.5782 - lr: 6.7032e-04\n",
      "Epoch 363/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3505 - accuracy: 0.8553\n",
      "Epoch 363: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.3495 - accuracy: 0.8557 - val_loss: 1.5773 - val_accuracy: 0.5901 - lr: 6.7032e-04\n",
      "Epoch 364/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3631 - accuracy: 0.8540\n",
      "Epoch 364: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.3624 - accuracy: 0.8536 - val_loss: 1.7360 - val_accuracy: 0.5850 - lr: 6.7032e-04\n",
      "Epoch 365/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3721 - accuracy: 0.8596\n",
      "Epoch 365: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.3719 - accuracy: 0.8595 - val_loss: 1.8630 - val_accuracy: 0.5187 - lr: 6.7032e-04\n",
      "Epoch 366/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3753 - accuracy: 0.8519\n",
      "Epoch 366: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.3749 - accuracy: 0.8519 - val_loss: 1.5128 - val_accuracy: 0.5901 - lr: 6.7032e-04\n",
      "Epoch 367/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3792 - accuracy: 0.8459\n",
      "Epoch 367: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.3786 - accuracy: 0.8467 - val_loss: 1.6136 - val_accuracy: 0.6122 - lr: 6.7032e-04\n",
      "Epoch 368/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3599 - accuracy: 0.8600\n",
      "Epoch 368: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.3597 - accuracy: 0.8604 - val_loss: 1.5893 - val_accuracy: 0.5935 - lr: 6.7032e-04\n",
      "Epoch 369/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3820 - accuracy: 0.8604\n",
      "Epoch 369: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.3807 - accuracy: 0.8612 - val_loss: 1.3961 - val_accuracy: 0.6173 - lr: 6.7032e-04\n",
      "Epoch 370/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3717 - accuracy: 0.8549\n",
      "Epoch 370: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.3721 - accuracy: 0.8544 - val_loss: 2.2441 - val_accuracy: 0.4371 - lr: 6.7032e-04\n",
      "Epoch 371/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3552 - accuracy: 0.8596\n",
      "Epoch 371: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.3562 - accuracy: 0.8591 - val_loss: 2.0667 - val_accuracy: 0.5068 - lr: 6.7032e-04\n",
      "Epoch 372/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3737 - accuracy: 0.8519\n",
      "Epoch 372: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.3743 - accuracy: 0.8514 - val_loss: 1.8102 - val_accuracy: 0.5731 - lr: 6.7032e-04\n",
      "Epoch 373/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4012 - accuracy: 0.8438\n",
      "Epoch 373: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.4012 - accuracy: 0.8438 - val_loss: 1.5110 - val_accuracy: 0.6054 - lr: 6.7032e-04\n",
      "Epoch 374/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3881 - accuracy: 0.8485\n",
      "Epoch 374: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.3923 - accuracy: 0.8467 - val_loss: 2.0220 - val_accuracy: 0.5170 - lr: 6.7032e-04\n",
      "Epoch 375/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3560 - accuracy: 0.8652\n",
      "Epoch 375: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.3555 - accuracy: 0.8655 - val_loss: 1.4883 - val_accuracy: 0.5952 - lr: 6.7032e-04\n",
      "Epoch 376/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3767 - accuracy: 0.8523\n",
      "Epoch 376: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3767 - accuracy: 0.8523 - val_loss: 1.5108 - val_accuracy: 0.5884 - lr: 6.7032e-04\n",
      "Epoch 377/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3834 - accuracy: 0.8557\n",
      "Epoch 377: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3834 - accuracy: 0.8557 - val_loss: 2.0321 - val_accuracy: 0.5374 - lr: 6.7032e-04\n",
      "Epoch 378/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3970 - accuracy: 0.8433\n",
      "Epoch 378: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3970 - accuracy: 0.8433 - val_loss: 1.5608 - val_accuracy: 0.6003 - lr: 6.7032e-04\n",
      "Epoch 379/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3908 - accuracy: 0.8446\n",
      "Epoch 379: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3908 - accuracy: 0.8446 - val_loss: 1.6155 - val_accuracy: 0.5697 - lr: 6.7032e-04\n",
      "Epoch 380/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.4014 - accuracy: 0.8497\n",
      "Epoch 380: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.4002 - accuracy: 0.8501 - val_loss: 2.0286 - val_accuracy: 0.5493 - lr: 6.7032e-04\n",
      "Epoch 381/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3773 - accuracy: 0.8646\n",
      "Epoch 381: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.3773 - accuracy: 0.8646 - val_loss: 1.5130 - val_accuracy: 0.5646 - lr: 6.7032e-04\n",
      "Epoch 382/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3735 - accuracy: 0.8480\n",
      "Epoch 382: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 83ms/step - loss: 0.3735 - accuracy: 0.8480 - val_loss: 2.3155 - val_accuracy: 0.5102 - lr: 6.7032e-04\n",
      "Epoch 383/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3949 - accuracy: 0.8510\n",
      "Epoch 383: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3949 - accuracy: 0.8510 - val_loss: 1.5049 - val_accuracy: 0.5867 - lr: 6.7032e-04\n",
      "Epoch 384/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3528 - accuracy: 0.8587\n",
      "Epoch 384: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3528 - accuracy: 0.8587 - val_loss: 1.7269 - val_accuracy: 0.6088 - lr: 6.7032e-04\n",
      "Epoch 385/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3811 - accuracy: 0.8523\n",
      "Epoch 385: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 78ms/step - loss: 0.3811 - accuracy: 0.8523 - val_loss: 1.7496 - val_accuracy: 0.5867 - lr: 6.7032e-04\n",
      "Epoch 386/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3717 - accuracy: 0.8519\n",
      "Epoch 386: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3713 - accuracy: 0.8514 - val_loss: 1.6684 - val_accuracy: 0.5612 - lr: 6.7032e-04\n",
      "Epoch 387/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3960 - accuracy: 0.8493\n",
      "Epoch 387: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3960 - accuracy: 0.8493 - val_loss: 1.5129 - val_accuracy: 0.5969 - lr: 6.7032e-04\n",
      "Epoch 388/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3690 - accuracy: 0.8540\n",
      "Epoch 388: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3707 - accuracy: 0.8540 - val_loss: 1.5231 - val_accuracy: 0.6020 - lr: 6.7032e-04\n",
      "Epoch 389/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3559 - accuracy: 0.8582\n",
      "Epoch 389: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3559 - accuracy: 0.8582 - val_loss: 1.6141 - val_accuracy: 0.5612 - lr: 6.7032e-04\n",
      "Epoch 390/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3702 - accuracy: 0.8497\n",
      "Epoch 390: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3702 - accuracy: 0.8497 - val_loss: 1.4825 - val_accuracy: 0.6156 - lr: 6.7032e-04\n",
      "Epoch 391/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3526 - accuracy: 0.8604\n",
      "Epoch 391: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.3526 - accuracy: 0.8604 - val_loss: 1.4976 - val_accuracy: 0.6088 - lr: 6.7032e-04\n",
      "Epoch 392/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3854 - accuracy: 0.8455\n",
      "Epoch 392: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3854 - accuracy: 0.8455 - val_loss: 1.7781 - val_accuracy: 0.5680 - lr: 6.7032e-04\n",
      "Epoch 393/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3502 - accuracy: 0.8642\n",
      "Epoch 393: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.3502 - accuracy: 0.8642 - val_loss: 1.7251 - val_accuracy: 0.5612 - lr: 6.7032e-04\n",
      "Epoch 394/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3606 - accuracy: 0.8612\n",
      "Epoch 394: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.3606 - accuracy: 0.8612 - val_loss: 1.4668 - val_accuracy: 0.6054 - lr: 6.7032e-04\n",
      "Epoch 395/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3715 - accuracy: 0.8485\n",
      "Epoch 395: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.3703 - accuracy: 0.8489 - val_loss: 1.6154 - val_accuracy: 0.5731 - lr: 6.7032e-04\n",
      "Epoch 396/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3849 - accuracy: 0.8467\n",
      "Epoch 396: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.3843 - accuracy: 0.8472 - val_loss: 1.3856 - val_accuracy: 0.5799 - lr: 6.7032e-04\n",
      "Epoch 397/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3650 - accuracy: 0.8629\n",
      "Epoch 397: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.3650 - accuracy: 0.8629 - val_loss: 1.5275 - val_accuracy: 0.5952 - lr: 6.7032e-04\n",
      "Epoch 398/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3628 - accuracy: 0.8638\n",
      "Epoch 398: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3628 - accuracy: 0.8638 - val_loss: 1.5767 - val_accuracy: 0.5629 - lr: 6.7032e-04\n",
      "Epoch 399/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3668 - accuracy: 0.8544\n",
      "Epoch 399: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3668 - accuracy: 0.8544 - val_loss: 1.6533 - val_accuracy: 0.5918 - lr: 6.7032e-04\n",
      "Epoch 400/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3796 - accuracy: 0.8616\n",
      "Epoch 400: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.3796 - accuracy: 0.8616 - val_loss: 1.6540 - val_accuracy: 0.6122 - lr: 6.7032e-04\n",
      "Epoch 401/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3555 - accuracy: 0.8612\n",
      "Epoch 401: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.3555 - accuracy: 0.8612 - val_loss: 1.5774 - val_accuracy: 0.5884 - lr: 6.0653e-04\n",
      "Epoch 402/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3507 - accuracy: 0.8613\n",
      "Epoch 402: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.3500 - accuracy: 0.8612 - val_loss: 1.5489 - val_accuracy: 0.6037 - lr: 6.0653e-04\n",
      "Epoch 403/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3599 - accuracy: 0.8587\n",
      "Epoch 403: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.3599 - accuracy: 0.8587 - val_loss: 1.7147 - val_accuracy: 0.6105 - lr: 6.0653e-04\n",
      "Epoch 404/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3524 - accuracy: 0.8660\n",
      "Epoch 404: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.3513 - accuracy: 0.8668 - val_loss: 1.5890 - val_accuracy: 0.6224 - lr: 6.0653e-04\n",
      "Epoch 405/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3506 - accuracy: 0.8659\n",
      "Epoch 405: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.3506 - accuracy: 0.8659 - val_loss: 1.7124 - val_accuracy: 0.5935 - lr: 6.0653e-04\n",
      "Epoch 406/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3500 - accuracy: 0.8626\n",
      "Epoch 406: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.3501 - accuracy: 0.8621 - val_loss: 1.5811 - val_accuracy: 0.6071 - lr: 6.0653e-04\n",
      "Epoch 407/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3713 - accuracy: 0.8515\n",
      "Epoch 407: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.3713 - accuracy: 0.8514 - val_loss: 1.6975 - val_accuracy: 0.5850 - lr: 6.0653e-04\n",
      "Epoch 408/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3402 - accuracy: 0.8642\n",
      "Epoch 408: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.3402 - accuracy: 0.8642 - val_loss: 1.7131 - val_accuracy: 0.5765 - lr: 6.0653e-04\n",
      "Epoch 409/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3792 - accuracy: 0.8565\n",
      "Epoch 409: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.3792 - accuracy: 0.8565 - val_loss: 1.5740 - val_accuracy: 0.6003 - lr: 6.0653e-04\n",
      "Epoch 410/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.8536\n",
      "Epoch 410: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3787 - accuracy: 0.8531 - val_loss: 1.6870 - val_accuracy: 0.5986 - lr: 6.0653e-04\n",
      "Epoch 411/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3417 - accuracy: 0.8714\n",
      "Epoch 411: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3417 - accuracy: 0.8714 - val_loss: 2.0233 - val_accuracy: 0.5782 - lr: 6.0653e-04\n",
      "Epoch 412/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3691 - accuracy: 0.8612\n",
      "Epoch 412: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3691 - accuracy: 0.8612 - val_loss: 1.6177 - val_accuracy: 0.5986 - lr: 6.0653e-04\n",
      "Epoch 413/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3336 - accuracy: 0.8731\n",
      "Epoch 413: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3336 - accuracy: 0.8731 - val_loss: 1.6595 - val_accuracy: 0.5918 - lr: 6.0653e-04\n",
      "Epoch 414/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3574 - accuracy: 0.8650\n",
      "Epoch 414: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3574 - accuracy: 0.8650 - val_loss: 1.5096 - val_accuracy: 0.6173 - lr: 6.0653e-04\n",
      "Epoch 415/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3287 - accuracy: 0.8753\n",
      "Epoch 415: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3287 - accuracy: 0.8753 - val_loss: 1.5004 - val_accuracy: 0.6020 - lr: 6.0653e-04\n",
      "Epoch 416/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3392 - accuracy: 0.8714\n",
      "Epoch 416: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3392 - accuracy: 0.8714 - val_loss: 1.5624 - val_accuracy: 0.6003 - lr: 6.0653e-04\n",
      "Epoch 417/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3424 - accuracy: 0.8638\n",
      "Epoch 417: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3424 - accuracy: 0.8638 - val_loss: 1.7673 - val_accuracy: 0.5918 - lr: 6.0653e-04\n",
      "Epoch 418/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3688 - accuracy: 0.8557\n",
      "Epoch 418: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3688 - accuracy: 0.8557 - val_loss: 1.6175 - val_accuracy: 0.5918 - lr: 6.0653e-04\n",
      "Epoch 419/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3188 - accuracy: 0.8817\n",
      "Epoch 419: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3188 - accuracy: 0.8817 - val_loss: 2.0859 - val_accuracy: 0.5051 - lr: 6.0653e-04\n",
      "Epoch 420/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3668 - accuracy: 0.8638\n",
      "Epoch 420: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3668 - accuracy: 0.8638 - val_loss: 1.8830 - val_accuracy: 0.5816 - lr: 6.0653e-04\n",
      "Epoch 421/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3673 - accuracy: 0.8514\n",
      "Epoch 421: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3673 - accuracy: 0.8514 - val_loss: 2.4349 - val_accuracy: 0.4830 - lr: 6.0653e-04\n",
      "Epoch 422/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3545 - accuracy: 0.8591\n",
      "Epoch 422: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3545 - accuracy: 0.8591 - val_loss: 1.4175 - val_accuracy: 0.5952 - lr: 6.0653e-04\n",
      "Epoch 423/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3443 - accuracy: 0.8655\n",
      "Epoch 423: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3443 - accuracy: 0.8655 - val_loss: 1.7931 - val_accuracy: 0.5527 - lr: 6.0653e-04\n",
      "Epoch 424/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3570 - accuracy: 0.8625\n",
      "Epoch 424: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3570 - accuracy: 0.8625 - val_loss: 1.4053 - val_accuracy: 0.6139 - lr: 6.0653e-04\n",
      "Epoch 425/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3417 - accuracy: 0.8697\n",
      "Epoch 425: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 78ms/step - loss: 0.3417 - accuracy: 0.8697 - val_loss: 1.4179 - val_accuracy: 0.6088 - lr: 6.0653e-04\n",
      "Epoch 426/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3360 - accuracy: 0.8642\n",
      "Epoch 426: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3360 - accuracy: 0.8642 - val_loss: 1.9189 - val_accuracy: 0.5527 - lr: 6.0653e-04\n",
      "Epoch 427/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3278 - accuracy: 0.8778\n",
      "Epoch 427: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3278 - accuracy: 0.8778 - val_loss: 1.7276 - val_accuracy: 0.5986 - lr: 6.0653e-04\n",
      "Epoch 428/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3382 - accuracy: 0.8719\n",
      "Epoch 428: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3382 - accuracy: 0.8719 - val_loss: 2.1721 - val_accuracy: 0.5323 - lr: 6.0653e-04\n",
      "Epoch 429/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3431 - accuracy: 0.8685\n",
      "Epoch 429: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3431 - accuracy: 0.8685 - val_loss: 1.5284 - val_accuracy: 0.6122 - lr: 6.0653e-04\n",
      "Epoch 430/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3446 - accuracy: 0.8680\n",
      "Epoch 430: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3446 - accuracy: 0.8680 - val_loss: 1.6228 - val_accuracy: 0.5969 - lr: 6.0653e-04\n",
      "Epoch 431/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3340 - accuracy: 0.8680\n",
      "Epoch 431: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.3340 - accuracy: 0.8680 - val_loss: 1.6302 - val_accuracy: 0.5748 - lr: 6.0653e-04\n",
      "Epoch 432/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3421 - accuracy: 0.8697\n",
      "Epoch 432: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.3421 - accuracy: 0.8697 - val_loss: 1.6842 - val_accuracy: 0.6020 - lr: 6.0653e-04\n",
      "Epoch 433/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3646 - accuracy: 0.8663\n",
      "Epoch 433: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.3646 - accuracy: 0.8663 - val_loss: 2.1204 - val_accuracy: 0.5017 - lr: 6.0653e-04\n",
      "Epoch 434/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3244 - accuracy: 0.8736\n",
      "Epoch 434: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.3244 - accuracy: 0.8736 - val_loss: 1.6812 - val_accuracy: 0.6088 - lr: 6.0653e-04\n",
      "Epoch 435/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3266 - accuracy: 0.8778\n",
      "Epoch 435: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.3266 - accuracy: 0.8778 - val_loss: 1.6950 - val_accuracy: 0.6122 - lr: 6.0653e-04\n",
      "Epoch 436/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3434 - accuracy: 0.8693\n",
      "Epoch 436: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.3434 - accuracy: 0.8693 - val_loss: 1.5928 - val_accuracy: 0.6173 - lr: 6.0653e-04\n",
      "Epoch 437/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3226 - accuracy: 0.8744\n",
      "Epoch 437: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.3226 - accuracy: 0.8744 - val_loss: 1.4317 - val_accuracy: 0.6105 - lr: 6.0653e-04\n",
      "Epoch 438/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3347 - accuracy: 0.8736\n",
      "Epoch 438: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.3347 - accuracy: 0.8736 - val_loss: 1.7932 - val_accuracy: 0.5612 - lr: 6.0653e-04\n",
      "Epoch 439/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3696 - accuracy: 0.8548\n",
      "Epoch 439: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3696 - accuracy: 0.8548 - val_loss: 1.5837 - val_accuracy: 0.6071 - lr: 6.0653e-04\n",
      "Epoch 440/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3132 - accuracy: 0.8791\n",
      "Epoch 440: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3132 - accuracy: 0.8791 - val_loss: 1.6784 - val_accuracy: 0.6224 - lr: 6.0653e-04\n",
      "Epoch 441/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3047 - accuracy: 0.8872\n",
      "Epoch 441: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3047 - accuracy: 0.8872 - val_loss: 2.0349 - val_accuracy: 0.5850 - lr: 6.0653e-04\n",
      "Epoch 442/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3400 - accuracy: 0.8663\n",
      "Epoch 442: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.3400 - accuracy: 0.8663 - val_loss: 1.7621 - val_accuracy: 0.5969 - lr: 6.0653e-04\n",
      "Epoch 443/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3604 - accuracy: 0.8582\n",
      "Epoch 443: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3604 - accuracy: 0.8582 - val_loss: 1.5073 - val_accuracy: 0.6003 - lr: 6.0653e-04\n",
      "Epoch 444/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3230 - accuracy: 0.8774\n",
      "Epoch 444: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3230 - accuracy: 0.8774 - val_loss: 1.4998 - val_accuracy: 0.6190 - lr: 6.0653e-04\n",
      "Epoch 445/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3570 - accuracy: 0.8574\n",
      "Epoch 445: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 78ms/step - loss: 0.3570 - accuracy: 0.8574 - val_loss: 1.9951 - val_accuracy: 0.5748 - lr: 6.0653e-04\n",
      "Epoch 446/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3393 - accuracy: 0.8689\n",
      "Epoch 446: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3393 - accuracy: 0.8689 - val_loss: 1.5630 - val_accuracy: 0.6105 - lr: 6.0653e-04\n",
      "Epoch 447/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3437 - accuracy: 0.8680\n",
      "Epoch 447: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.3437 - accuracy: 0.8680 - val_loss: 1.9676 - val_accuracy: 0.5680 - lr: 6.0653e-04\n",
      "Epoch 448/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3406 - accuracy: 0.8689\n",
      "Epoch 448: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.3406 - accuracy: 0.8689 - val_loss: 1.6760 - val_accuracy: 0.5850 - lr: 6.0653e-04\n",
      "Epoch 449/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3303 - accuracy: 0.8706\n",
      "Epoch 449: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 13s 85ms/step - loss: 0.3303 - accuracy: 0.8706 - val_loss: 1.6899 - val_accuracy: 0.5714 - lr: 6.0653e-04\n",
      "Epoch 450/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3496 - accuracy: 0.8680\n",
      "Epoch 450: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.3496 - accuracy: 0.8680 - val_loss: 1.8660 - val_accuracy: 0.5578 - lr: 6.0653e-04\n",
      "Epoch 451/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3405 - accuracy: 0.8706\n",
      "Epoch 451: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.3405 - accuracy: 0.8706 - val_loss: 1.8517 - val_accuracy: 0.5476 - lr: 6.0653e-04\n",
      "Epoch 452/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3337 - accuracy: 0.8748\n",
      "Epoch 452: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.3337 - accuracy: 0.8748 - val_loss: 1.6612 - val_accuracy: 0.6003 - lr: 6.0653e-04\n",
      "Epoch 453/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3812 - accuracy: 0.8604\n",
      "Epoch 453: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 83ms/step - loss: 0.3812 - accuracy: 0.8604 - val_loss: 1.6673 - val_accuracy: 0.5833 - lr: 6.0653e-04\n",
      "Epoch 454/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3323 - accuracy: 0.8727\n",
      "Epoch 454: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 13s 87ms/step - loss: 0.3323 - accuracy: 0.8727 - val_loss: 2.4363 - val_accuracy: 0.5510 - lr: 6.0653e-04\n",
      "Epoch 455/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3590 - accuracy: 0.8608\n",
      "Epoch 455: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 13s 88ms/step - loss: 0.3590 - accuracy: 0.8608 - val_loss: 1.7023 - val_accuracy: 0.5986 - lr: 6.0653e-04\n",
      "Epoch 456/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3501 - accuracy: 0.8629\n",
      "Epoch 456: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.3501 - accuracy: 0.8629 - val_loss: 1.8535 - val_accuracy: 0.5782 - lr: 6.0653e-04\n",
      "Epoch 457/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3432 - accuracy: 0.8676\n",
      "Epoch 457: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.3432 - accuracy: 0.8676 - val_loss: 1.6599 - val_accuracy: 0.5595 - lr: 6.0653e-04\n",
      "Epoch 458/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3410 - accuracy: 0.8659\n",
      "Epoch 458: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.3410 - accuracy: 0.8659 - val_loss: 1.9881 - val_accuracy: 0.5578 - lr: 6.0653e-04\n",
      "Epoch 459/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3304 - accuracy: 0.8668\n",
      "Epoch 459: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.3304 - accuracy: 0.8668 - val_loss: 1.6654 - val_accuracy: 0.6071 - lr: 6.0653e-04\n",
      "Epoch 460/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3499 - accuracy: 0.8663\n",
      "Epoch 460: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.3499 - accuracy: 0.8663 - val_loss: 1.5790 - val_accuracy: 0.6105 - lr: 6.0653e-04\n",
      "Epoch 461/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3375 - accuracy: 0.8706\n",
      "Epoch 461: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.3375 - accuracy: 0.8706 - val_loss: 1.4074 - val_accuracy: 0.6122 - lr: 6.0653e-04\n",
      "Epoch 462/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3318 - accuracy: 0.8646\n",
      "Epoch 462: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.3318 - accuracy: 0.8646 - val_loss: 1.7964 - val_accuracy: 0.5867 - lr: 6.0653e-04\n",
      "Epoch 463/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3380 - accuracy: 0.8710\n",
      "Epoch 463: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.3380 - accuracy: 0.8710 - val_loss: 1.7565 - val_accuracy: 0.5986 - lr: 6.0653e-04\n",
      "Epoch 464/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3094 - accuracy: 0.8868\n",
      "Epoch 464: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.3094 - accuracy: 0.8868 - val_loss: 1.6786 - val_accuracy: 0.5765 - lr: 6.0653e-04\n",
      "Epoch 465/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3045 - accuracy: 0.8795\n",
      "Epoch 465: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3045 - accuracy: 0.8795 - val_loss: 1.6972 - val_accuracy: 0.6037 - lr: 6.0653e-04\n",
      "Epoch 466/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3253 - accuracy: 0.8727\n",
      "Epoch 466: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3253 - accuracy: 0.8727 - val_loss: 2.1403 - val_accuracy: 0.5272 - lr: 6.0653e-04\n",
      "Epoch 467/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3402 - accuracy: 0.8604\n",
      "Epoch 467: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3402 - accuracy: 0.8604 - val_loss: 1.5196 - val_accuracy: 0.6071 - lr: 6.0653e-04\n",
      "Epoch 468/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3474 - accuracy: 0.8625\n",
      "Epoch 468: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3474 - accuracy: 0.8625 - val_loss: 1.6688 - val_accuracy: 0.6020 - lr: 6.0653e-04\n",
      "Epoch 469/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3332 - accuracy: 0.8702\n",
      "Epoch 469: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3332 - accuracy: 0.8702 - val_loss: 2.3012 - val_accuracy: 0.5561 - lr: 6.0653e-04\n",
      "Epoch 470/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2929 - accuracy: 0.8889\n",
      "Epoch 470: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.2929 - accuracy: 0.8889 - val_loss: 1.9861 - val_accuracy: 0.5459 - lr: 6.0653e-04\n",
      "Epoch 471/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3303 - accuracy: 0.8693\n",
      "Epoch 471: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3303 - accuracy: 0.8693 - val_loss: 1.7110 - val_accuracy: 0.6139 - lr: 6.0653e-04\n",
      "Epoch 472/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3020 - accuracy: 0.8851\n",
      "Epoch 472: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3020 - accuracy: 0.8851 - val_loss: 1.6095 - val_accuracy: 0.6207 - lr: 6.0653e-04\n",
      "Epoch 473/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3312 - accuracy: 0.8693\n",
      "Epoch 473: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3312 - accuracy: 0.8693 - val_loss: 1.7404 - val_accuracy: 0.6020 - lr: 6.0653e-04\n",
      "Epoch 474/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3248 - accuracy: 0.8748\n",
      "Epoch 474: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.3248 - accuracy: 0.8748 - val_loss: 1.9457 - val_accuracy: 0.5748 - lr: 6.0653e-04\n",
      "Epoch 475/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3335 - accuracy: 0.8770\n",
      "Epoch 475: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3335 - accuracy: 0.8770 - val_loss: 1.7762 - val_accuracy: 0.6054 - lr: 6.0653e-04\n",
      "Epoch 476/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3402 - accuracy: 0.8731\n",
      "Epoch 476: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3402 - accuracy: 0.8731 - val_loss: 2.1423 - val_accuracy: 0.5425 - lr: 6.0653e-04\n",
      "Epoch 477/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3321 - accuracy: 0.8740\n",
      "Epoch 477: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3321 - accuracy: 0.8740 - val_loss: 1.7834 - val_accuracy: 0.5850 - lr: 6.0653e-04\n",
      "Epoch 478/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3291 - accuracy: 0.8727\n",
      "Epoch 478: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3291 - accuracy: 0.8727 - val_loss: 1.8261 - val_accuracy: 0.5969 - lr: 6.0653e-04\n",
      "Epoch 479/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3047 - accuracy: 0.8872\n",
      "Epoch 479: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3047 - accuracy: 0.8872 - val_loss: 1.7177 - val_accuracy: 0.5952 - lr: 6.0653e-04\n",
      "Epoch 480/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3072 - accuracy: 0.8799\n",
      "Epoch 480: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3072 - accuracy: 0.8799 - val_loss: 2.0283 - val_accuracy: 0.6020 - lr: 6.0653e-04\n",
      "Epoch 481/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3302 - accuracy: 0.8757\n",
      "Epoch 481: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3302 - accuracy: 0.8757 - val_loss: 1.5948 - val_accuracy: 0.6105 - lr: 6.0653e-04\n",
      "Epoch 482/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3498 - accuracy: 0.8685\n",
      "Epoch 482: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.3498 - accuracy: 0.8685 - val_loss: 2.2441 - val_accuracy: 0.5612 - lr: 6.0653e-04\n",
      "Epoch 483/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3515 - accuracy: 0.8646\n",
      "Epoch 483: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 13s 85ms/step - loss: 0.3515 - accuracy: 0.8646 - val_loss: 1.8631 - val_accuracy: 0.5374 - lr: 6.0653e-04\n",
      "Epoch 484/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3208 - accuracy: 0.8702\n",
      "Epoch 484: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 84ms/step - loss: 0.3208 - accuracy: 0.8702 - val_loss: 1.5493 - val_accuracy: 0.5833 - lr: 6.0653e-04\n",
      "Epoch 485/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3314 - accuracy: 0.8680\n",
      "Epoch 485: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 13s 87ms/step - loss: 0.3314 - accuracy: 0.8680 - val_loss: 1.6616 - val_accuracy: 0.5816 - lr: 6.0653e-04\n",
      "Epoch 486/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3401 - accuracy: 0.8736\n",
      "Epoch 486: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 84ms/step - loss: 0.3401 - accuracy: 0.8736 - val_loss: 1.9708 - val_accuracy: 0.5731 - lr: 6.0653e-04\n",
      "Epoch 487/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3189 - accuracy: 0.8778\n",
      "Epoch 487: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 84ms/step - loss: 0.3189 - accuracy: 0.8778 - val_loss: 2.1506 - val_accuracy: 0.5697 - lr: 6.0653e-04\n",
      "Epoch 488/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3686 - accuracy: 0.8561\n",
      "Epoch 488: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 84ms/step - loss: 0.3686 - accuracy: 0.8561 - val_loss: 3.1344 - val_accuracy: 0.4762 - lr: 6.0653e-04\n",
      "Epoch 489/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3101 - accuracy: 0.8846\n",
      "Epoch 489: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 84ms/step - loss: 0.3101 - accuracy: 0.8846 - val_loss: 1.5843 - val_accuracy: 0.6071 - lr: 6.0653e-04\n",
      "Epoch 490/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3416 - accuracy: 0.8723\n",
      "Epoch 490: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 84ms/step - loss: 0.3416 - accuracy: 0.8723 - val_loss: 1.6083 - val_accuracy: 0.6241 - lr: 6.0653e-04\n",
      "Epoch 491/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3323 - accuracy: 0.8650\n",
      "Epoch 491: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 84ms/step - loss: 0.3323 - accuracy: 0.8650 - val_loss: 1.5250 - val_accuracy: 0.6224 - lr: 6.0653e-04\n",
      "Epoch 492/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3639 - accuracy: 0.8638\n",
      "Epoch 492: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 84ms/step - loss: 0.3639 - accuracy: 0.8638 - val_loss: 1.9046 - val_accuracy: 0.5765 - lr: 6.0653e-04\n",
      "Epoch 493/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2959 - accuracy: 0.8889\n",
      "Epoch 493: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.2959 - accuracy: 0.8889 - val_loss: 1.8003 - val_accuracy: 0.6105 - lr: 6.0653e-04\n",
      "Epoch 494/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3164 - accuracy: 0.8842\n",
      "Epoch 494: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3164 - accuracy: 0.8842 - val_loss: 1.5533 - val_accuracy: 0.6037 - lr: 6.0653e-04\n",
      "Epoch 495/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3228 - accuracy: 0.8706\n",
      "Epoch 495: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3228 - accuracy: 0.8706 - val_loss: 1.7816 - val_accuracy: 0.6054 - lr: 6.0653e-04\n",
      "Epoch 496/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3168 - accuracy: 0.8799\n",
      "Epoch 496: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3168 - accuracy: 0.8799 - val_loss: 1.9236 - val_accuracy: 0.5833 - lr: 6.0653e-04\n",
      "Epoch 497/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3020 - accuracy: 0.8791\n",
      "Epoch 497: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.3020 - accuracy: 0.8791 - val_loss: 2.1619 - val_accuracy: 0.5527 - lr: 6.0653e-04\n",
      "Epoch 498/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3354 - accuracy: 0.8719\n",
      "Epoch 498: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 12s 78ms/step - loss: 0.3354 - accuracy: 0.8719 - val_loss: 2.1151 - val_accuracy: 0.5782 - lr: 6.0653e-04\n",
      "Epoch 499/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3387 - accuracy: 0.8676\n",
      "Epoch 499: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3387 - accuracy: 0.8676 - val_loss: 1.8079 - val_accuracy: 0.6020 - lr: 6.0653e-04\n",
      "Epoch 500/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3618 - accuracy: 0.8604\n",
      "Epoch 500: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3618 - accuracy: 0.8604 - val_loss: 1.9278 - val_accuracy: 0.5884 - lr: 6.0653e-04\n",
      "Epoch 501/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3105 - accuracy: 0.8817\n",
      "Epoch 501: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3105 - accuracy: 0.8817 - val_loss: 1.9863 - val_accuracy: 0.5323 - lr: 5.4881e-04\n",
      "Epoch 502/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3207 - accuracy: 0.8791\n",
      "Epoch 502: val_accuracy did not improve from 0.62415\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3207 - accuracy: 0.8791 - val_loss: 2.2552 - val_accuracy: 0.5663 - lr: 5.4881e-04\n",
      "Epoch 503/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3152 - accuracy: 0.8789\n",
      "Epoch 503: val_accuracy improved from 0.62415 to 0.62755, saving model to ./models/ResNet50_Dense_add.hdf5\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3172 - accuracy: 0.8778 - val_loss: 1.4635 - val_accuracy: 0.6276 - lr: 5.4881e-04\n",
      "Epoch 504/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3179 - accuracy: 0.8765\n",
      "Epoch 504: val_accuracy did not improve from 0.62755\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3179 - accuracy: 0.8765 - val_loss: 1.5087 - val_accuracy: 0.6088 - lr: 5.4881e-04\n",
      "Epoch 505/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2967 - accuracy: 0.8868\n",
      "Epoch 505: val_accuracy improved from 0.62755 to 0.64626, saving model to ./models/ResNet50_Dense_add.hdf5\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.2967 - accuracy: 0.8868 - val_loss: 1.5942 - val_accuracy: 0.6463 - lr: 5.4881e-04\n",
      "Epoch 506/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3062 - accuracy: 0.8859\n",
      "Epoch 506: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3062 - accuracy: 0.8859 - val_loss: 2.5475 - val_accuracy: 0.5119 - lr: 5.4881e-04\n",
      "Epoch 507/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3190 - accuracy: 0.8757\n",
      "Epoch 507: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3190 - accuracy: 0.8757 - val_loss: 1.6695 - val_accuracy: 0.6139 - lr: 5.4881e-04\n",
      "Epoch 508/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3426 - accuracy: 0.8685\n",
      "Epoch 508: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3426 - accuracy: 0.8685 - val_loss: 1.9291 - val_accuracy: 0.6071 - lr: 5.4881e-04\n",
      "Epoch 509/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3275 - accuracy: 0.8744\n",
      "Epoch 509: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.3275 - accuracy: 0.8744 - val_loss: 1.7911 - val_accuracy: 0.5850 - lr: 5.4881e-04\n",
      "Epoch 510/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3072 - accuracy: 0.8842\n",
      "Epoch 510: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3072 - accuracy: 0.8842 - val_loss: 1.6577 - val_accuracy: 0.5969 - lr: 5.4881e-04\n",
      "Epoch 511/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3029 - accuracy: 0.8829\n",
      "Epoch 511: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3029 - accuracy: 0.8829 - val_loss: 1.5746 - val_accuracy: 0.6173 - lr: 5.4881e-04\n",
      "Epoch 512/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3117 - accuracy: 0.8817\n",
      "Epoch 512: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3117 - accuracy: 0.8817 - val_loss: 1.9888 - val_accuracy: 0.5748 - lr: 5.4881e-04\n",
      "Epoch 513/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3028 - accuracy: 0.8791\n",
      "Epoch 513: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3028 - accuracy: 0.8791 - val_loss: 2.1160 - val_accuracy: 0.6003 - lr: 5.4881e-04\n",
      "Epoch 514/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3226 - accuracy: 0.8731\n",
      "Epoch 514: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3226 - accuracy: 0.8731 - val_loss: 1.4970 - val_accuracy: 0.6037 - lr: 5.4881e-04\n",
      "Epoch 515/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3036 - accuracy: 0.8791\n",
      "Epoch 515: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3036 - accuracy: 0.8791 - val_loss: 1.6447 - val_accuracy: 0.5748 - lr: 5.4881e-04\n",
      "Epoch 516/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3203 - accuracy: 0.8770\n",
      "Epoch 516: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3203 - accuracy: 0.8770 - val_loss: 2.0960 - val_accuracy: 0.6037 - lr: 5.4881e-04\n",
      "Epoch 517/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3092 - accuracy: 0.8731\n",
      "Epoch 517: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3092 - accuracy: 0.8731 - val_loss: 2.5870 - val_accuracy: 0.5357 - lr: 5.4881e-04\n",
      "Epoch 518/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3055 - accuracy: 0.8821\n",
      "Epoch 518: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3055 - accuracy: 0.8821 - val_loss: 1.6228 - val_accuracy: 0.6122 - lr: 5.4881e-04\n",
      "Epoch 519/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3045 - accuracy: 0.8753\n",
      "Epoch 519: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3045 - accuracy: 0.8753 - val_loss: 1.6556 - val_accuracy: 0.5952 - lr: 5.4881e-04\n",
      "Epoch 520/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2948 - accuracy: 0.8897\n",
      "Epoch 520: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2948 - accuracy: 0.8897 - val_loss: 1.8301 - val_accuracy: 0.6122 - lr: 5.4881e-04\n",
      "Epoch 521/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3188 - accuracy: 0.8799\n",
      "Epoch 521: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3188 - accuracy: 0.8799 - val_loss: 1.6845 - val_accuracy: 0.6224 - lr: 5.4881e-04\n",
      "Epoch 522/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2811 - accuracy: 0.8868\n",
      "Epoch 522: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.2811 - accuracy: 0.8868 - val_loss: 2.0403 - val_accuracy: 0.5714 - lr: 5.4881e-04\n",
      "Epoch 523/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2952 - accuracy: 0.8859\n",
      "Epoch 523: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2952 - accuracy: 0.8859 - val_loss: 1.7625 - val_accuracy: 0.5748 - lr: 5.4881e-04\n",
      "Epoch 524/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3494 - accuracy: 0.8685\n",
      "Epoch 524: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3494 - accuracy: 0.8685 - val_loss: 2.0385 - val_accuracy: 0.5816 - lr: 5.4881e-04\n",
      "Epoch 525/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3132 - accuracy: 0.8842\n",
      "Epoch 525: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3132 - accuracy: 0.8842 - val_loss: 1.7970 - val_accuracy: 0.6139 - lr: 5.4881e-04\n",
      "Epoch 526/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3278 - accuracy: 0.8787\n",
      "Epoch 526: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3278 - accuracy: 0.8787 - val_loss: 1.6459 - val_accuracy: 0.6020 - lr: 5.4881e-04\n",
      "Epoch 527/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3059 - accuracy: 0.8799\n",
      "Epoch 527: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3059 - accuracy: 0.8799 - val_loss: 1.6809 - val_accuracy: 0.6156 - lr: 5.4881e-04\n",
      "Epoch 528/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3117 - accuracy: 0.8740\n",
      "Epoch 528: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3117 - accuracy: 0.8740 - val_loss: 2.0072 - val_accuracy: 0.5816 - lr: 5.4881e-04\n",
      "Epoch 529/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2994 - accuracy: 0.8868\n",
      "Epoch 529: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2994 - accuracy: 0.8868 - val_loss: 1.9570 - val_accuracy: 0.5833 - lr: 5.4881e-04\n",
      "Epoch 530/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3066 - accuracy: 0.8804\n",
      "Epoch 530: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3066 - accuracy: 0.8804 - val_loss: 1.9588 - val_accuracy: 0.5833 - lr: 5.4881e-04\n",
      "Epoch 531/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3142 - accuracy: 0.8791\n",
      "Epoch 531: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3142 - accuracy: 0.8791 - val_loss: 1.5787 - val_accuracy: 0.6378 - lr: 5.4881e-04\n",
      "Epoch 532/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2977 - accuracy: 0.8821\n",
      "Epoch 532: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2977 - accuracy: 0.8821 - val_loss: 2.3687 - val_accuracy: 0.5153 - lr: 5.4881e-04\n",
      "Epoch 533/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3176 - accuracy: 0.8736\n",
      "Epoch 533: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3176 - accuracy: 0.8736 - val_loss: 1.6412 - val_accuracy: 0.6139 - lr: 5.4881e-04\n",
      "Epoch 534/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3146 - accuracy: 0.8829\n",
      "Epoch 534: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3146 - accuracy: 0.8829 - val_loss: 2.1175 - val_accuracy: 0.5714 - lr: 5.4881e-04\n",
      "Epoch 535/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.8851\n",
      "Epoch 535: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 78ms/step - loss: 0.2913 - accuracy: 0.8851 - val_loss: 2.0445 - val_accuracy: 0.5969 - lr: 5.4881e-04\n",
      "Epoch 536/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2919 - accuracy: 0.8906\n",
      "Epoch 536: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2919 - accuracy: 0.8906 - val_loss: 2.0852 - val_accuracy: 0.6020 - lr: 5.4881e-04\n",
      "Epoch 537/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2963 - accuracy: 0.8825\n",
      "Epoch 537: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2963 - accuracy: 0.8825 - val_loss: 1.8907 - val_accuracy: 0.5867 - lr: 5.4881e-04\n",
      "Epoch 538/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 0.8897\n",
      "Epoch 538: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2848 - accuracy: 0.8897 - val_loss: 1.7737 - val_accuracy: 0.5714 - lr: 5.4881e-04\n",
      "Epoch 539/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3079 - accuracy: 0.8736\n",
      "Epoch 539: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3079 - accuracy: 0.8736 - val_loss: 1.8695 - val_accuracy: 0.5765 - lr: 5.4881e-04\n",
      "Epoch 540/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3197 - accuracy: 0.8821\n",
      "Epoch 540: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3197 - accuracy: 0.8821 - val_loss: 1.9690 - val_accuracy: 0.6054 - lr: 5.4881e-04\n",
      "Epoch 541/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3205 - accuracy: 0.8838\n",
      "Epoch 541: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3205 - accuracy: 0.8838 - val_loss: 1.8062 - val_accuracy: 0.5969 - lr: 5.4881e-04\n",
      "Epoch 542/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3342 - accuracy: 0.8727\n",
      "Epoch 542: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3342 - accuracy: 0.8727 - val_loss: 1.9506 - val_accuracy: 0.6088 - lr: 5.4881e-04\n",
      "Epoch 543/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3006 - accuracy: 0.8851\n",
      "Epoch 543: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3006 - accuracy: 0.8851 - val_loss: 1.8300 - val_accuracy: 0.5697 - lr: 5.4881e-04\n",
      "Epoch 544/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2930 - accuracy: 0.8910\n",
      "Epoch 544: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2930 - accuracy: 0.8910 - val_loss: 1.8803 - val_accuracy: 0.6088 - lr: 5.4881e-04\n",
      "Epoch 545/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2780 - accuracy: 0.8885\n",
      "Epoch 545: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2780 - accuracy: 0.8885 - val_loss: 1.8570 - val_accuracy: 0.5952 - lr: 5.4881e-04\n",
      "Epoch 546/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3130 - accuracy: 0.8906\n",
      "Epoch 546: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3130 - accuracy: 0.8906 - val_loss: 1.7510 - val_accuracy: 0.5935 - lr: 5.4881e-04\n",
      "Epoch 547/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2713 - accuracy: 0.9021\n",
      "Epoch 547: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.2713 - accuracy: 0.9021 - val_loss: 1.7181 - val_accuracy: 0.6139 - lr: 5.4881e-04\n",
      "Epoch 548/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3420 - accuracy: 0.8697\n",
      "Epoch 548: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.3420 - accuracy: 0.8697 - val_loss: 1.8940 - val_accuracy: 0.5969 - lr: 5.4881e-04\n",
      "Epoch 549/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3077 - accuracy: 0.8757\n",
      "Epoch 549: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3077 - accuracy: 0.8757 - val_loss: 1.7536 - val_accuracy: 0.5969 - lr: 5.4881e-04\n",
      "Epoch 550/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2950 - accuracy: 0.8868\n",
      "Epoch 550: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2950 - accuracy: 0.8868 - val_loss: 1.7975 - val_accuracy: 0.6105 - lr: 5.4881e-04\n",
      "Epoch 551/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3219 - accuracy: 0.8778\n",
      "Epoch 551: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3219 - accuracy: 0.8778 - val_loss: 1.9637 - val_accuracy: 0.5663 - lr: 5.4881e-04\n",
      "Epoch 552/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3195 - accuracy: 0.8799\n",
      "Epoch 552: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3195 - accuracy: 0.8799 - val_loss: 2.1276 - val_accuracy: 0.5799 - lr: 5.4881e-04\n",
      "Epoch 553/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2890 - accuracy: 0.8846\n",
      "Epoch 553: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2890 - accuracy: 0.8846 - val_loss: 2.2641 - val_accuracy: 0.5493 - lr: 5.4881e-04\n",
      "Epoch 554/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3108 - accuracy: 0.8795\n",
      "Epoch 554: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3108 - accuracy: 0.8795 - val_loss: 1.8797 - val_accuracy: 0.5816 - lr: 5.4881e-04\n",
      "Epoch 555/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3216 - accuracy: 0.8821\n",
      "Epoch 555: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3216 - accuracy: 0.8821 - val_loss: 2.1284 - val_accuracy: 0.5833 - lr: 5.4881e-04\n",
      "Epoch 556/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2919 - accuracy: 0.8851\n",
      "Epoch 556: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2919 - accuracy: 0.8851 - val_loss: 1.8082 - val_accuracy: 0.5969 - lr: 5.4881e-04\n",
      "Epoch 557/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3107 - accuracy: 0.8874\n",
      "Epoch 557: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3096 - accuracy: 0.8876 - val_loss: 2.2087 - val_accuracy: 0.5289 - lr: 5.4881e-04\n",
      "Epoch 558/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2929 - accuracy: 0.8919\n",
      "Epoch 558: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2929 - accuracy: 0.8919 - val_loss: 2.1654 - val_accuracy: 0.5714 - lr: 5.4881e-04\n",
      "Epoch 559/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2976 - accuracy: 0.8791\n",
      "Epoch 559: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2976 - accuracy: 0.8791 - val_loss: 1.6832 - val_accuracy: 0.5969 - lr: 5.4881e-04\n",
      "Epoch 560/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2953 - accuracy: 0.8842\n",
      "Epoch 560: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2953 - accuracy: 0.8842 - val_loss: 1.9135 - val_accuracy: 0.5884 - lr: 5.4881e-04\n",
      "Epoch 561/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3043 - accuracy: 0.8770\n",
      "Epoch 561: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3043 - accuracy: 0.8770 - val_loss: 2.3108 - val_accuracy: 0.5867 - lr: 5.4881e-04\n",
      "Epoch 562/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3067 - accuracy: 0.8897\n",
      "Epoch 562: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3067 - accuracy: 0.8897 - val_loss: 1.7584 - val_accuracy: 0.6122 - lr: 5.4881e-04\n",
      "Epoch 563/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.8878\n",
      "Epoch 563: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2877 - accuracy: 0.8885 - val_loss: 1.7239 - val_accuracy: 0.6190 - lr: 5.4881e-04\n",
      "Epoch 564/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3240 - accuracy: 0.8795\n",
      "Epoch 564: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3240 - accuracy: 0.8795 - val_loss: 2.4859 - val_accuracy: 0.5527 - lr: 5.4881e-04\n",
      "Epoch 565/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.2723 - accuracy: 0.8938\n",
      "Epoch 565: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2751 - accuracy: 0.8927 - val_loss: 1.7757 - val_accuracy: 0.6054 - lr: 5.4881e-04\n",
      "Epoch 566/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2959 - accuracy: 0.8846\n",
      "Epoch 566: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2959 - accuracy: 0.8846 - val_loss: 2.1968 - val_accuracy: 0.5867 - lr: 5.4881e-04\n",
      "Epoch 567/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2984 - accuracy: 0.8782\n",
      "Epoch 567: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2984 - accuracy: 0.8782 - val_loss: 1.7650 - val_accuracy: 0.6122 - lr: 5.4881e-04\n",
      "Epoch 568/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3204 - accuracy: 0.8795\n",
      "Epoch 568: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3204 - accuracy: 0.8795 - val_loss: 1.9377 - val_accuracy: 0.5901 - lr: 5.4881e-04\n",
      "Epoch 569/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.3220 - accuracy: 0.8797\n",
      "Epoch 569: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3216 - accuracy: 0.8799 - val_loss: 1.6948 - val_accuracy: 0.6088 - lr: 5.4881e-04\n",
      "Epoch 570/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2753 - accuracy: 0.8948\n",
      "Epoch 570: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2753 - accuracy: 0.8948 - val_loss: 2.0334 - val_accuracy: 0.6037 - lr: 5.4881e-04\n",
      "Epoch 571/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.2909 - accuracy: 0.8921\n",
      "Epoch 571: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2911 - accuracy: 0.8914 - val_loss: 1.8698 - val_accuracy: 0.5918 - lr: 5.4881e-04\n",
      "Epoch 572/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2667 - accuracy: 0.8983\n",
      "Epoch 572: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2667 - accuracy: 0.8983 - val_loss: 1.6568 - val_accuracy: 0.6156 - lr: 5.4881e-04\n",
      "Epoch 573/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2879 - accuracy: 0.8931\n",
      "Epoch 573: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.2879 - accuracy: 0.8931 - val_loss: 2.7095 - val_accuracy: 0.5306 - lr: 5.4881e-04\n",
      "Epoch 574/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3282 - accuracy: 0.8757\n",
      "Epoch 574: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 78ms/step - loss: 0.3282 - accuracy: 0.8757 - val_loss: 2.4171 - val_accuracy: 0.5459 - lr: 5.4881e-04\n",
      "Epoch 575/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3296 - accuracy: 0.8787\n",
      "Epoch 575: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3296 - accuracy: 0.8787 - val_loss: 1.8970 - val_accuracy: 0.5629 - lr: 5.4881e-04\n",
      "Epoch 576/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2903 - accuracy: 0.8885\n",
      "Epoch 576: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.2903 - accuracy: 0.8885 - val_loss: 1.8774 - val_accuracy: 0.5986 - lr: 5.4881e-04\n",
      "Epoch 577/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3059 - accuracy: 0.8834\n",
      "Epoch 577: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.3059 - accuracy: 0.8834 - val_loss: 1.7374 - val_accuracy: 0.6139 - lr: 5.4881e-04\n",
      "Epoch 578/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2916 - accuracy: 0.8902\n",
      "Epoch 578: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.2916 - accuracy: 0.8902 - val_loss: 1.7294 - val_accuracy: 0.6276 - lr: 5.4881e-04\n",
      "Epoch 579/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2876 - accuracy: 0.8889\n",
      "Epoch 579: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.2876 - accuracy: 0.8889 - val_loss: 1.8485 - val_accuracy: 0.5731 - lr: 5.4881e-04\n",
      "Epoch 580/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.8880\n",
      "Epoch 580: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.2779 - accuracy: 0.8880 - val_loss: 1.8335 - val_accuracy: 0.6259 - lr: 5.4881e-04\n",
      "Epoch 581/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2808 - accuracy: 0.8902\n",
      "Epoch 581: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.2808 - accuracy: 0.8902 - val_loss: 1.7365 - val_accuracy: 0.5969 - lr: 5.4881e-04\n",
      "Epoch 582/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3131 - accuracy: 0.8782\n",
      "Epoch 582: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.3131 - accuracy: 0.8782 - val_loss: 1.9013 - val_accuracy: 0.5952 - lr: 5.4881e-04\n",
      "Epoch 583/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3070 - accuracy: 0.8834\n",
      "Epoch 583: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3070 - accuracy: 0.8834 - val_loss: 2.2724 - val_accuracy: 0.6139 - lr: 5.4881e-04\n",
      "Epoch 584/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3156 - accuracy: 0.8761\n",
      "Epoch 584: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 13s 86ms/step - loss: 0.3156 - accuracy: 0.8761 - val_loss: 1.9503 - val_accuracy: 0.5969 - lr: 5.4881e-04\n",
      "Epoch 585/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2831 - accuracy: 0.8872\n",
      "Epoch 585: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 84ms/step - loss: 0.2831 - accuracy: 0.8872 - val_loss: 1.9524 - val_accuracy: 0.6088 - lr: 5.4881e-04\n",
      "Epoch 586/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3186 - accuracy: 0.8748\n",
      "Epoch 586: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 13s 87ms/step - loss: 0.3186 - accuracy: 0.8748 - val_loss: 1.9078 - val_accuracy: 0.5935 - lr: 5.4881e-04\n",
      "Epoch 587/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2992 - accuracy: 0.8846\n",
      "Epoch 587: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.2992 - accuracy: 0.8846 - val_loss: 1.9669 - val_accuracy: 0.5969 - lr: 5.4881e-04\n",
      "Epoch 588/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3106 - accuracy: 0.8812\n",
      "Epoch 588: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.3106 - accuracy: 0.8812 - val_loss: 2.3881 - val_accuracy: 0.5731 - lr: 5.4881e-04\n",
      "Epoch 589/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2998 - accuracy: 0.8834\n",
      "Epoch 589: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2998 - accuracy: 0.8834 - val_loss: 1.9528 - val_accuracy: 0.5714 - lr: 5.4881e-04\n",
      "Epoch 590/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2762 - accuracy: 0.8936\n",
      "Epoch 590: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2762 - accuracy: 0.8936 - val_loss: 1.7309 - val_accuracy: 0.6037 - lr: 5.4881e-04\n",
      "Epoch 591/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2891 - accuracy: 0.8876\n",
      "Epoch 591: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2891 - accuracy: 0.8876 - val_loss: 2.2293 - val_accuracy: 0.5612 - lr: 5.4881e-04\n",
      "Epoch 592/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3026 - accuracy: 0.8765\n",
      "Epoch 592: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3026 - accuracy: 0.8765 - val_loss: 1.9753 - val_accuracy: 0.6139 - lr: 5.4881e-04\n",
      "Epoch 593/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2891 - accuracy: 0.8808\n",
      "Epoch 593: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2891 - accuracy: 0.8808 - val_loss: 1.7857 - val_accuracy: 0.6139 - lr: 5.4881e-04\n",
      "Epoch 594/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3036 - accuracy: 0.8855\n",
      "Epoch 594: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3036 - accuracy: 0.8855 - val_loss: 2.7635 - val_accuracy: 0.5170 - lr: 5.4881e-04\n",
      "Epoch 595/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3163 - accuracy: 0.8868\n",
      "Epoch 595: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3163 - accuracy: 0.8868 - val_loss: 1.7238 - val_accuracy: 0.5935 - lr: 5.4881e-04\n",
      "Epoch 596/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3014 - accuracy: 0.8825\n",
      "Epoch 596: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.3014 - accuracy: 0.8825 - val_loss: 2.0373 - val_accuracy: 0.6156 - lr: 5.4881e-04\n",
      "Epoch 597/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3073 - accuracy: 0.8829\n",
      "Epoch 597: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.3073 - accuracy: 0.8829 - val_loss: 1.8290 - val_accuracy: 0.5697 - lr: 5.4881e-04\n",
      "Epoch 598/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2985 - accuracy: 0.8838\n",
      "Epoch 598: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2985 - accuracy: 0.8838 - val_loss: 1.9422 - val_accuracy: 0.5408 - lr: 5.4881e-04\n",
      "Epoch 599/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2877 - accuracy: 0.8885\n",
      "Epoch 599: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2877 - accuracy: 0.8885 - val_loss: 1.7625 - val_accuracy: 0.5867 - lr: 5.4881e-04\n",
      "Epoch 600/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2677 - accuracy: 0.8910\n",
      "Epoch 600: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2677 - accuracy: 0.8910 - val_loss: 2.0944 - val_accuracy: 0.6088 - lr: 5.4881e-04\n",
      "Epoch 601/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2617 - accuracy: 0.8983\n",
      "Epoch 601: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2617 - accuracy: 0.8983 - val_loss: 1.6900 - val_accuracy: 0.6105 - lr: 4.9659e-04\n",
      "Epoch 602/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2684 - accuracy: 0.9017\n",
      "Epoch 602: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2684 - accuracy: 0.9017 - val_loss: 1.7406 - val_accuracy: 0.5952 - lr: 4.9659e-04\n",
      "Epoch 603/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2811 - accuracy: 0.8906\n",
      "Epoch 603: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2811 - accuracy: 0.8906 - val_loss: 1.8990 - val_accuracy: 0.5680 - lr: 4.9659e-04\n",
      "Epoch 604/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3052 - accuracy: 0.8842\n",
      "Epoch 604: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.3052 - accuracy: 0.8842 - val_loss: 2.3292 - val_accuracy: 0.5867 - lr: 4.9659e-04\n",
      "Epoch 605/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2746 - accuracy: 0.8863\n",
      "Epoch 605: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2746 - accuracy: 0.8863 - val_loss: 2.2015 - val_accuracy: 0.6122 - lr: 4.9659e-04\n",
      "Epoch 606/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2725 - accuracy: 0.9008\n",
      "Epoch 606: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2725 - accuracy: 0.9008 - val_loss: 1.7237 - val_accuracy: 0.6088 - lr: 4.9659e-04\n",
      "Epoch 607/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2886 - accuracy: 0.8897\n",
      "Epoch 607: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2886 - accuracy: 0.8897 - val_loss: 2.4999 - val_accuracy: 0.5765 - lr: 4.9659e-04\n",
      "Epoch 608/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2687 - accuracy: 0.8970\n",
      "Epoch 608: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2687 - accuracy: 0.8970 - val_loss: 2.2903 - val_accuracy: 0.5867 - lr: 4.9659e-04\n",
      "Epoch 609/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2793 - accuracy: 0.8957\n",
      "Epoch 609: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.2793 - accuracy: 0.8957 - val_loss: 2.2232 - val_accuracy: 0.6037 - lr: 4.9659e-04\n",
      "Epoch 610/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2712 - accuracy: 0.8953\n",
      "Epoch 610: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2712 - accuracy: 0.8953 - val_loss: 1.7370 - val_accuracy: 0.6190 - lr: 4.9659e-04\n",
      "Epoch 611/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2817 - accuracy: 0.8936\n",
      "Epoch 611: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2817 - accuracy: 0.8936 - val_loss: 1.9129 - val_accuracy: 0.6071 - lr: 4.9659e-04\n",
      "Epoch 612/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2714 - accuracy: 0.8931\n",
      "Epoch 612: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2714 - accuracy: 0.8931 - val_loss: 1.7870 - val_accuracy: 0.6139 - lr: 4.9659e-04\n",
      "Epoch 613/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2711 - accuracy: 0.8902\n",
      "Epoch 613: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2711 - accuracy: 0.8902 - val_loss: 2.4287 - val_accuracy: 0.5782 - lr: 4.9659e-04\n",
      "Epoch 614/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2833 - accuracy: 0.8885\n",
      "Epoch 614: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2833 - accuracy: 0.8885 - val_loss: 1.7374 - val_accuracy: 0.5935 - lr: 4.9659e-04\n",
      "Epoch 615/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2741 - accuracy: 0.8948\n",
      "Epoch 615: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.2741 - accuracy: 0.8948 - val_loss: 2.3407 - val_accuracy: 0.5765 - lr: 4.9659e-04\n",
      "Epoch 616/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2813 - accuracy: 0.8948\n",
      "Epoch 616: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.2813 - accuracy: 0.8948 - val_loss: 2.1397 - val_accuracy: 0.5850 - lr: 4.9659e-04\n",
      "Epoch 617/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2766 - accuracy: 0.8940\n",
      "Epoch 617: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.2766 - accuracy: 0.8940 - val_loss: 2.3156 - val_accuracy: 0.5646 - lr: 4.9659e-04\n",
      "Epoch 618/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2876 - accuracy: 0.8804\n",
      "Epoch 618: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.2876 - accuracy: 0.8804 - val_loss: 2.0545 - val_accuracy: 0.6156 - lr: 4.9659e-04\n",
      "Epoch 619/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2769 - accuracy: 0.8991\n",
      "Epoch 619: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.2769 - accuracy: 0.8991 - val_loss: 2.0473 - val_accuracy: 0.5901 - lr: 4.9659e-04\n",
      "Epoch 620/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2660 - accuracy: 0.8978\n",
      "Epoch 620: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.2660 - accuracy: 0.8978 - val_loss: 1.8889 - val_accuracy: 0.6105 - lr: 4.9659e-04\n",
      "Epoch 621/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.8983\n",
      "Epoch 621: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.2498 - accuracy: 0.8983 - val_loss: 1.9030 - val_accuracy: 0.6020 - lr: 4.9659e-04\n",
      "Epoch 622/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2758 - accuracy: 0.8953\n",
      "Epoch 622: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.2758 - accuracy: 0.8953 - val_loss: 1.9996 - val_accuracy: 0.6190 - lr: 4.9659e-04\n",
      "Epoch 623/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2784 - accuracy: 0.8825\n",
      "Epoch 623: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.2784 - accuracy: 0.8825 - val_loss: 2.0267 - val_accuracy: 0.5935 - lr: 4.9659e-04\n",
      "Epoch 624/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2907 - accuracy: 0.8893\n",
      "Epoch 624: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.2907 - accuracy: 0.8893 - val_loss: 1.8889 - val_accuracy: 0.5765 - lr: 4.9659e-04\n",
      "Epoch 625/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3029 - accuracy: 0.8821\n",
      "Epoch 625: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.3029 - accuracy: 0.8821 - val_loss: 1.8221 - val_accuracy: 0.6190 - lr: 4.9659e-04\n",
      "Epoch 626/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2773 - accuracy: 0.8936\n",
      "Epoch 626: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.2773 - accuracy: 0.8936 - val_loss: 1.9123 - val_accuracy: 0.5833 - lr: 4.9659e-04\n",
      "Epoch 627/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2765 - accuracy: 0.8944\n",
      "Epoch 627: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.2765 - accuracy: 0.8944 - val_loss: 2.2641 - val_accuracy: 0.5884 - lr: 4.9659e-04\n",
      "Epoch 628/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2726 - accuracy: 0.8983\n",
      "Epoch 628: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.2726 - accuracy: 0.8983 - val_loss: 1.8084 - val_accuracy: 0.6003 - lr: 4.9659e-04\n",
      "Epoch 629/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2699 - accuracy: 0.9000\n",
      "Epoch 629: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.2699 - accuracy: 0.9000 - val_loss: 1.9918 - val_accuracy: 0.6037 - lr: 4.9659e-04\n",
      "Epoch 630/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2741 - accuracy: 0.8940\n",
      "Epoch 630: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.2741 - accuracy: 0.8940 - val_loss: 1.9518 - val_accuracy: 0.6105 - lr: 4.9659e-04\n",
      "Epoch 631/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2620 - accuracy: 0.8987\n",
      "Epoch 631: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.2620 - accuracy: 0.8987 - val_loss: 2.1678 - val_accuracy: 0.5595 - lr: 4.9659e-04\n",
      "Epoch 632/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2939 - accuracy: 0.8825\n",
      "Epoch 632: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.2939 - accuracy: 0.8825 - val_loss: 2.1926 - val_accuracy: 0.5748 - lr: 4.9659e-04\n",
      "Epoch 633/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2819 - accuracy: 0.8880\n",
      "Epoch 633: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 84ms/step - loss: 0.2819 - accuracy: 0.8880 - val_loss: 2.1524 - val_accuracy: 0.5527 - lr: 4.9659e-04\n",
      "Epoch 634/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2934 - accuracy: 0.8863\n",
      "Epoch 634: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.2934 - accuracy: 0.8863 - val_loss: 1.9830 - val_accuracy: 0.6190 - lr: 4.9659e-04\n",
      "Epoch 635/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2766 - accuracy: 0.9008\n",
      "Epoch 635: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 83ms/step - loss: 0.2766 - accuracy: 0.9008 - val_loss: 3.3164 - val_accuracy: 0.5374 - lr: 4.9659e-04\n",
      "Epoch 636/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2983 - accuracy: 0.8897\n",
      "Epoch 636: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 84ms/step - loss: 0.2983 - accuracy: 0.8897 - val_loss: 1.9036 - val_accuracy: 0.6276 - lr: 4.9659e-04\n",
      "Epoch 637/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3036 - accuracy: 0.8906\n",
      "Epoch 637: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 83ms/step - loss: 0.3036 - accuracy: 0.8906 - val_loss: 1.7670 - val_accuracy: 0.6054 - lr: 4.9659e-04\n",
      "Epoch 638/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2832 - accuracy: 0.8897\n",
      "Epoch 638: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 13s 85ms/step - loss: 0.2832 - accuracy: 0.8897 - val_loss: 1.8624 - val_accuracy: 0.6088 - lr: 4.9659e-04\n",
      "Epoch 639/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2816 - accuracy: 0.8893\n",
      "Epoch 639: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 82ms/step - loss: 0.2816 - accuracy: 0.8893 - val_loss: 2.0489 - val_accuracy: 0.6020 - lr: 4.9659e-04\n",
      "Epoch 640/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.9000\n",
      "Epoch 640: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2640 - accuracy: 0.9000 - val_loss: 2.1804 - val_accuracy: 0.6139 - lr: 4.9659e-04\n",
      "Epoch 641/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2591 - accuracy: 0.9042\n",
      "Epoch 641: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2591 - accuracy: 0.9042 - val_loss: 1.7797 - val_accuracy: 0.6071 - lr: 4.9659e-04\n",
      "Epoch 642/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2676 - accuracy: 0.8983\n",
      "Epoch 642: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.2676 - accuracy: 0.8983 - val_loss: 1.9155 - val_accuracy: 0.5918 - lr: 4.9659e-04\n",
      "Epoch 643/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2629 - accuracy: 0.8987\n",
      "Epoch 643: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.2629 - accuracy: 0.8987 - val_loss: 1.9612 - val_accuracy: 0.5799 - lr: 4.9659e-04\n",
      "Epoch 644/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2734 - accuracy: 0.8940\n",
      "Epoch 644: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.2734 - accuracy: 0.8940 - val_loss: 2.0849 - val_accuracy: 0.5867 - lr: 4.9659e-04\n",
      "Epoch 645/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.2837 - accuracy: 0.8930\n",
      "Epoch 645: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.2825 - accuracy: 0.8936 - val_loss: 2.1149 - val_accuracy: 0.6207 - lr: 4.9659e-04\n",
      "Epoch 646/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.2760 - accuracy: 0.8985\n",
      "Epoch 646: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.2758 - accuracy: 0.8987 - val_loss: 2.5133 - val_accuracy: 0.5697 - lr: 4.9659e-04\n",
      "Epoch 647/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.2585 - accuracy: 0.8981\n",
      "Epoch 647: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.2577 - accuracy: 0.8987 - val_loss: 2.0597 - val_accuracy: 0.5935 - lr: 4.9659e-04\n",
      "Epoch 648/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.2787 - accuracy: 0.8968\n",
      "Epoch 648: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.2798 - accuracy: 0.8970 - val_loss: 2.1146 - val_accuracy: 0.5935 - lr: 4.9659e-04\n",
      "Epoch 649/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2844 - accuracy: 0.8897\n",
      "Epoch 649: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 73ms/step - loss: 0.2844 - accuracy: 0.8897 - val_loss: 1.6979 - val_accuracy: 0.6190 - lr: 4.9659e-04\n",
      "Epoch 650/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2538 - accuracy: 0.9051\n",
      "Epoch 650: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 74ms/step - loss: 0.2538 - accuracy: 0.9051 - val_loss: 2.2975 - val_accuracy: 0.5986 - lr: 4.9659e-04\n",
      "Epoch 651/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.2987 - accuracy: 0.8943\n",
      "Epoch 651: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 75ms/step - loss: 0.2989 - accuracy: 0.8940 - val_loss: 2.0802 - val_accuracy: 0.5901 - lr: 4.9659e-04\n",
      "Epoch 652/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2885 - accuracy: 0.8910\n",
      "Epoch 652: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.2885 - accuracy: 0.8910 - val_loss: 2.2152 - val_accuracy: 0.5850 - lr: 4.9659e-04\n",
      "Epoch 653/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2958 - accuracy: 0.8872\n",
      "Epoch 653: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.2958 - accuracy: 0.8872 - val_loss: 2.5319 - val_accuracy: 0.5442 - lr: 4.9659e-04\n",
      "Epoch 654/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.8870\n",
      "Epoch 654: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2862 - accuracy: 0.8876 - val_loss: 1.8914 - val_accuracy: 0.6156 - lr: 4.9659e-04\n",
      "Epoch 655/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2520 - accuracy: 0.9025\n",
      "Epoch 655: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.2520 - accuracy: 0.9025 - val_loss: 1.7636 - val_accuracy: 0.6071 - lr: 4.9659e-04\n",
      "Epoch 656/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2721 - accuracy: 0.8978\n",
      "Epoch 656: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 13s 89ms/step - loss: 0.2721 - accuracy: 0.8978 - val_loss: 1.9740 - val_accuracy: 0.6037 - lr: 4.9659e-04\n",
      "Epoch 657/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2619 - accuracy: 0.8961\n",
      "Epoch 657: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.2619 - accuracy: 0.8961 - val_loss: 1.8942 - val_accuracy: 0.6241 - lr: 4.9659e-04\n",
      "Epoch 658/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2812 - accuracy: 0.8885\n",
      "Epoch 658: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.2812 - accuracy: 0.8885 - val_loss: 1.9883 - val_accuracy: 0.5969 - lr: 4.9659e-04\n",
      "Epoch 659/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2851 - accuracy: 0.9000\n",
      "Epoch 659: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.2851 - accuracy: 0.9000 - val_loss: 2.1120 - val_accuracy: 0.5765 - lr: 4.9659e-04\n",
      "Epoch 660/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2662 - accuracy: 0.8970\n",
      "Epoch 660: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2662 - accuracy: 0.8970 - val_loss: 1.8842 - val_accuracy: 0.6071 - lr: 4.9659e-04\n",
      "Epoch 661/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2783 - accuracy: 0.8966\n",
      "Epoch 661: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2783 - accuracy: 0.8966 - val_loss: 1.7639 - val_accuracy: 0.5986 - lr: 4.9659e-04\n",
      "Epoch 662/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.2948 - accuracy: 0.8818\n",
      "Epoch 662: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2946 - accuracy: 0.8817 - val_loss: 1.7428 - val_accuracy: 0.6224 - lr: 4.9659e-04\n",
      "Epoch 663/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2587 - accuracy: 0.9042\n",
      "Epoch 663: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 76ms/step - loss: 0.2587 - accuracy: 0.9042 - val_loss: 1.8914 - val_accuracy: 0.6088 - lr: 4.9659e-04\n",
      "Epoch 664/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2898 - accuracy: 0.8902\n",
      "Epoch 664: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2898 - accuracy: 0.8902 - val_loss: 1.8445 - val_accuracy: 0.5986 - lr: 4.9659e-04\n",
      "Epoch 665/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2765 - accuracy: 0.8961\n",
      "Epoch 665: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 13s 85ms/step - loss: 0.2765 - accuracy: 0.8961 - val_loss: 1.7936 - val_accuracy: 0.5986 - lr: 4.9659e-04\n",
      "Epoch 666/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2658 - accuracy: 0.8974\n",
      "Epoch 666: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 13s 88ms/step - loss: 0.2658 - accuracy: 0.8974 - val_loss: 2.1726 - val_accuracy: 0.6088 - lr: 4.9659e-04\n",
      "Epoch 667/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2685 - accuracy: 0.8957\n",
      "Epoch 667: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.2685 - accuracy: 0.8957 - val_loss: 1.7439 - val_accuracy: 0.6139 - lr: 4.9659e-04\n",
      "Epoch 668/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2690 - accuracy: 0.8961\n",
      "Epoch 668: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2690 - accuracy: 0.8961 - val_loss: 1.8863 - val_accuracy: 0.6037 - lr: 4.9659e-04\n",
      "Epoch 669/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2643 - accuracy: 0.8978\n",
      "Epoch 669: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.2643 - accuracy: 0.8978 - val_loss: 1.7382 - val_accuracy: 0.6327 - lr: 4.9659e-04\n",
      "Epoch 670/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2712 - accuracy: 0.8944\n",
      "Epoch 670: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2712 - accuracy: 0.8944 - val_loss: 1.8385 - val_accuracy: 0.6173 - lr: 4.9659e-04\n",
      "Epoch 671/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2776 - accuracy: 0.8919\n",
      "Epoch 671: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.2776 - accuracy: 0.8919 - val_loss: 1.9452 - val_accuracy: 0.6173 - lr: 4.9659e-04\n",
      "Epoch 672/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2741 - accuracy: 0.8872\n",
      "Epoch 672: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2741 - accuracy: 0.8872 - val_loss: 1.9051 - val_accuracy: 0.6105 - lr: 4.9659e-04\n",
      "Epoch 673/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2986 - accuracy: 0.8885\n",
      "Epoch 673: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2986 - accuracy: 0.8885 - val_loss: 1.9986 - val_accuracy: 0.5901 - lr: 4.9659e-04\n",
      "Epoch 674/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2762 - accuracy: 0.8944\n",
      "Epoch 674: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2762 - accuracy: 0.8944 - val_loss: 2.3389 - val_accuracy: 0.5901 - lr: 4.9659e-04\n",
      "Epoch 675/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2615 - accuracy: 0.9034\n",
      "Epoch 675: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2615 - accuracy: 0.9034 - val_loss: 1.9131 - val_accuracy: 0.5901 - lr: 4.9659e-04\n",
      "Epoch 676/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2764 - accuracy: 0.8936\n",
      "Epoch 676: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2764 - accuracy: 0.8936 - val_loss: 1.8650 - val_accuracy: 0.6327 - lr: 4.9659e-04\n",
      "Epoch 677/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2884 - accuracy: 0.8927\n",
      "Epoch 677: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2884 - accuracy: 0.8927 - val_loss: 1.7025 - val_accuracy: 0.6020 - lr: 4.9659e-04\n",
      "Epoch 678/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2462 - accuracy: 0.9051\n",
      "Epoch 678: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 81ms/step - loss: 0.2462 - accuracy: 0.9051 - val_loss: 1.9066 - val_accuracy: 0.5884 - lr: 4.9659e-04\n",
      "Epoch 679/1000\n",
      "146/147 [============================>.] - ETA: 0s - loss: 0.2376 - accuracy: 0.9088\n",
      "Epoch 679: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.2380 - accuracy: 0.9089 - val_loss: 1.8475 - val_accuracy: 0.6071 - lr: 4.9659e-04\n",
      "Epoch 680/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2689 - accuracy: 0.8987\n",
      "Epoch 680: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.2689 - accuracy: 0.8987 - val_loss: 2.0260 - val_accuracy: 0.6003 - lr: 4.9659e-04\n",
      "Epoch 681/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2953 - accuracy: 0.8906\n",
      "Epoch 681: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 83ms/step - loss: 0.2953 - accuracy: 0.8906 - val_loss: 1.9063 - val_accuracy: 0.5901 - lr: 4.9659e-04\n",
      "Epoch 682/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2481 - accuracy: 0.9046\n",
      "Epoch 682: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2481 - accuracy: 0.9046 - val_loss: 2.1368 - val_accuracy: 0.6293 - lr: 4.9659e-04\n",
      "Epoch 683/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2727 - accuracy: 0.8868\n",
      "Epoch 683: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2727 - accuracy: 0.8868 - val_loss: 2.0157 - val_accuracy: 0.6224 - lr: 4.9659e-04\n",
      "Epoch 684/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2819 - accuracy: 0.8872\n",
      "Epoch 684: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2819 - accuracy: 0.8872 - val_loss: 2.0135 - val_accuracy: 0.6003 - lr: 4.9659e-04\n",
      "Epoch 685/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2575 - accuracy: 0.9008\n",
      "Epoch 685: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2575 - accuracy: 0.9008 - val_loss: 2.0459 - val_accuracy: 0.6139 - lr: 4.9659e-04\n",
      "Epoch 686/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.8902\n",
      "Epoch 686: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2779 - accuracy: 0.8902 - val_loss: 1.8295 - val_accuracy: 0.6293 - lr: 4.9659e-04\n",
      "Epoch 687/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2562 - accuracy: 0.9102\n",
      "Epoch 687: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2562 - accuracy: 0.9102 - val_loss: 2.1747 - val_accuracy: 0.5867 - lr: 4.9659e-04\n",
      "Epoch 688/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2681 - accuracy: 0.8991\n",
      "Epoch 688: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2681 - accuracy: 0.8991 - val_loss: 2.2018 - val_accuracy: 0.5884 - lr: 4.9659e-04\n",
      "Epoch 689/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2733 - accuracy: 0.8987\n",
      "Epoch 689: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2733 - accuracy: 0.8987 - val_loss: 2.1715 - val_accuracy: 0.6037 - lr: 4.9659e-04\n",
      "Epoch 690/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2914 - accuracy: 0.8966\n",
      "Epoch 690: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2914 - accuracy: 0.8966 - val_loss: 1.9438 - val_accuracy: 0.6327 - lr: 4.9659e-04\n",
      "Epoch 691/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2815 - accuracy: 0.9093\n",
      "Epoch 691: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2815 - accuracy: 0.9093 - val_loss: 1.8911 - val_accuracy: 0.5986 - lr: 4.9659e-04\n",
      "Epoch 692/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2802 - accuracy: 0.8974\n",
      "Epoch 692: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2802 - accuracy: 0.8974 - val_loss: 1.7857 - val_accuracy: 0.6224 - lr: 4.9659e-04\n",
      "Epoch 693/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2580 - accuracy: 0.9042\n",
      "Epoch 693: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 80ms/step - loss: 0.2580 - accuracy: 0.9042 - val_loss: 1.7427 - val_accuracy: 0.6122 - lr: 4.9659e-04\n",
      "Epoch 694/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.9068\n",
      "Epoch 694: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2490 - accuracy: 0.9068 - val_loss: 1.7452 - val_accuracy: 0.6156 - lr: 4.9659e-04\n",
      "Epoch 695/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.9042\n",
      "Epoch 695: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2498 - accuracy: 0.9042 - val_loss: 1.8770 - val_accuracy: 0.6105 - lr: 4.9659e-04\n",
      "Epoch 696/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2905 - accuracy: 0.8978\n",
      "Epoch 696: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2905 - accuracy: 0.8978 - val_loss: 2.3565 - val_accuracy: 0.5918 - lr: 4.9659e-04\n",
      "Epoch 697/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2317 - accuracy: 0.9076\n",
      "Epoch 697: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2317 - accuracy: 0.9076 - val_loss: 1.9019 - val_accuracy: 0.6293 - lr: 4.9659e-04\n",
      "Epoch 698/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2442 - accuracy: 0.9038\n",
      "Epoch 698: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2442 - accuracy: 0.9038 - val_loss: 2.1090 - val_accuracy: 0.6071 - lr: 4.9659e-04\n",
      "Epoch 699/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.9093\n",
      "Epoch 699: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 77ms/step - loss: 0.2484 - accuracy: 0.9093 - val_loss: 1.9502 - val_accuracy: 0.6020 - lr: 4.9659e-04\n",
      "Epoch 700/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2784 - accuracy: 0.9051\n",
      "Epoch 700: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2784 - accuracy: 0.9051 - val_loss: 1.8520 - val_accuracy: 0.6088 - lr: 4.9659e-04\n",
      "Epoch 701/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2539 - accuracy: 0.8970\n",
      "Epoch 701: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 11s 78ms/step - loss: 0.2539 - accuracy: 0.8970 - val_loss: 1.8410 - val_accuracy: 0.6276 - lr: 4.4933e-04\n",
      "Epoch 702/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2584 - accuracy: 0.9046\n",
      "Epoch 702: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 83ms/step - loss: 0.2584 - accuracy: 0.9046 - val_loss: 1.8870 - val_accuracy: 0.6173 - lr: 4.4933e-04\n",
      "Epoch 703/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2447 - accuracy: 0.9046\n",
      "Epoch 703: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 78ms/step - loss: 0.2447 - accuracy: 0.9046 - val_loss: 1.9774 - val_accuracy: 0.6122 - lr: 4.4933e-04\n",
      "Epoch 704/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2942 - accuracy: 0.8957\n",
      "Epoch 704: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 78ms/step - loss: 0.2942 - accuracy: 0.8957 - val_loss: 1.9192 - val_accuracy: 0.6224 - lr: 4.4933e-04\n",
      "Epoch 705/1000\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2728 - accuracy: 0.8983\n",
      "Epoch 705: val_accuracy did not improve from 0.64626\n",
      "147/147 [==============================] - 12s 79ms/step - loss: 0.2728 - accuracy: 0.8983 - val_loss: 1.9591 - val_accuracy: 0.6020 - lr: 4.4933e-04\n",
      "Epoch 705: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.applications import ResNet50\n",
    "transfer_model = ResNet50(weights='imagenet',include_top=False, input_shape=(48,48,3))\n",
    "transfer_model.trainable = False\n",
    "\n",
    "finietune_model = Sequential(name='ResNet50_Dense_add')\n",
    "finietune_model.add(transfer_model)\n",
    "finietune_model.add(Flatten())\n",
    "finietune_model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "finietune_model.add(BatchNormalization())\n",
    "finietune_model.add(Activation('relu'))\n",
    "finietune_model.add(Dropout(0.5))\n",
    "finietune_model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "finietune_model.add(BatchNormalization())\n",
    "finietune_model.add(Activation('relu'))\n",
    "finietune_model.add(Dropout(0.5))\n",
    "finietune_model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "finietune_model.add(BatchNormalization())\n",
    "finietune_model.add(Activation('relu'))\n",
    "finietune_model.add(Dropout(0.5))\n",
    "finietune_model.add(Dense(3,activation='softmax'))\n",
    "\n",
    "plot_model(finietune_model,to_file=f'{folderpath}{finietune_model.name}.png',show_layer_names=True,show_layer_activations=True,show_shapes=True)\n",
    "\n",
    "erCB = EarlyStopping(verbose=1,patience=200,monitor='val_accuracy')\n",
    "mcCB = ModelCheckpoint(folderpath+f\"{finietune_model.name}.hdf5\",save_best_only=True,monitor='val_accuracy',verbose=1)\n",
    "\n",
    "def scheduler(epoch,lr=0.1):\n",
    "    if epoch % 100 == 0:\n",
    "        return lr * np.math.exp(-0.1)\n",
    "    else:\n",
    "        return lr\n",
    "lrCB = LearningRateScheduler(schedule=scheduler)\n",
    "\n",
    "finietune_model.compile(loss='categorical_crossentropy',optimizer='adam',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "history = finietune_model.fit(imgdata,eraLabel,batch_size=16,epochs=1000,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[erCB,mcCB,lrCB])\n",
    "\n",
    "\n",
    "\n",
    "# 검증셋과 학습셋의 오차를 저장\n",
    "y_vAcc = history.history['val_accuracy']\n",
    "y_Acc = history.history['accuracy']\n",
    "\n",
    "# 그래프로 표현해 봅니다\n",
    "x_len = np.arange(len(y_Acc))\n",
    "plt.plot(x_len,y_vAcc,marker='.',c='red',label='Test_acc')\n",
    "plt.plot(x_len,y_Acc,marker='.',c='blue',label='Train_acc')\n",
    "\n",
    "# 그래프에 그리드를 주고 레이블 표시\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.savefig(f'././image/{finietune_model.name}_acc.jpg')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 검증셋과 학습셋의 오차를 저장\n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "# 그래프로 표현해 봅니다\n",
    "x_len = np.arange(len(y_Acc))\n",
    "plt.plot(x_len,y_vloss,marker='.',c='red',label='Test_loss')\n",
    "plt.plot(x_len,y_loss,marker='.',c='blue',label='Train_loss')\n",
    "\n",
    "# 그래프에 그리드를 주고 레이블 표시\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig(f'././image/{finietune_model.name}_loss.jpg')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}